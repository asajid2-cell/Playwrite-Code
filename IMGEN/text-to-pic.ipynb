{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe5a74a",
   "metadata": {},
   "source": [
    "\n",
    "# Text-to-Pic: Lightweight Text-to-Image Trainer\n",
    "Build and train a tiny text-to-image generator that fits inside ~1 GB RAM at inference while still being able to learn from multiple captioned image corpora. The notebook downloads or ingests several lightweight datasets, sets up a compact diffusion backbone, and walks through training, evaluation, and export steps you can run locally (RTX 3070 Ti) before deploying to your site.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd80e7",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook Map\n",
    "1. Environment + dependency prep\n",
    "2. Config (model size, datasets, logging)\n",
    "3. Data ingest utilities (Hugging Face auto-discovery + local folders)\n",
    "4. Tiny UNet + text encoder wiring with `diffusers`\n",
    "5. Training loop w/ `accelerate`\n",
    "6. Sampling utilities + qualitative eval grid\n",
    "7. Export hints for 1 GB RAM web deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b16453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: z:\\Code\n",
      "Python: 3.11.0 | PyTorch: 2.5.1+cu121\n",
      "CUDA device: NVIDIA GeForce RTX 3070 Ti | capability (8, 6)\n",
      "Total VRAM: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def describe_env():\n",
    "    cwd = Path.cwd()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Working dir: {cwd}\")\n",
    "    print(f\"Python: {platform.python_version()} | PyTorch: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)} | capability {torch.cuda.get_device_capability(0)}\")\n",
    "        print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not detected - training will fall back to CPU (slow).\")\n",
    "\n",
    "describe_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the environment is missing dependencies, uncomment the next cell and run it once.\n",
    "# %pip install -q --upgrade torch torchvision diffusers transformers datasets accelerate Pillow einops matplotlib tensorboard huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de61a5",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Configuration\n",
    "Tweak the dataclasses below to control dataset mix, image size, training length, data discovery, and deployment-specific knobs. Defaults keep memory small (48x48 RGB outputs, Bert-tiny text encoder, mini UNet) so the exported weights load within ~900 MB when kept in fp16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823fad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Image size: 128px, total datasets: 3\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class DatasetSpec:\n",
    "    name: str\n",
    "    subset: Optional[str] = None\n",
    "    split: str = \"train\"\n",
    "    image_column: str = \"auto\"\n",
    "    caption_column: str = \"auto\"\n",
    "    max_samples: Optional[int] = None\n",
    "    weight: float = 1.0\n",
    "    type: str = \"huggingface\"  # or \"imagefolder\"\n",
    "    local_dir: Optional[str] = None\n",
    "    streaming: bool = False\n",
    "    trust_remote_code: bool = False\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    project_name: str = \"text-to-pic\"\n",
    "    output_dir: str = \"outputs/text_to_pic\"\n",
    "    checkpoint_dir: str = \"checkpoints/text_to_pic\"\n",
    "    sample_dir: str = \"outputs/text_to_pic/samples\"\n",
    "    data_cache_dir: str = \"data/hf_text2pic\"\n",
    "    seed: int = 42\n",
    "    image_size: int = 128\n",
    "    train_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    num_epochs: int = 32\n",
    "    max_train_steps: int = 32000\n",
    "    learning_rate: float = 5e-5\n",
    "    lr_warmup_steps: int = 2000\n",
    "    weight_decay: float = 5e-3\n",
    "    mixed_precision: str = \"fp16\"  # fp16/bf16/no\n",
    "    num_workers: int = 0\n",
    "    max_grad_norm: float = 0.8\n",
    "    num_inference_steps: int = 40\n",
    "    guidance_scale: float = 5.5\n",
    "    save_every: int = 2000\n",
    "    eval_every: int = 4000\n",
    "    max_prompt_length: int = 77\n",
    "    tokenizer_name: str = \"prajjwal1/bert-tiny\"\n",
    "    text_encoder_name: str = \"prajjwal1/bert-tiny\"\n",
    "    noise_steps: int = 1000\n",
    "    use_auto_dataset_search: bool = True\n",
    "    auto_dataset_search: str = \"text-to-image\"\n",
    "    auto_dataset_limit: int = 6\n",
    "    auto_dataset_max_samples: int = 8000\n",
    "    dataset_specs: List[DatasetSpec] = field(default_factory=lambda: [\n",
    "        DatasetSpec(\n",
    "            name=\"lambdalabs/naruto-blip-captions\",\n",
    "            image_column=\"image\",\n",
    "            caption_column=\"text\",\n",
    "            max_samples=12000,\n",
    "            weight=0.1,\n",
    "        ),\n",
    "        DatasetSpec(\n",
    "            name=\"poloclub/diffusiondb\",\n",
    "            subset=\"2m_first_1k\",\n",
    "            split=\"train\",\n",
    "            image_column=\"image\",\n",
    "            caption_column=\"prompt\",\n",
    "            max_samples=20000,\n",
    "            weight=0.45,\n",
    "            streaming=True,\n",
    "            trust_remote_code=True,\n",
    "        ),\n",
    "        DatasetSpec(\n",
    "            name=\"conceptual_captions\",\n",
    "            split=\"train\",\n",
    "            image_column=\"image\",\n",
    "            caption_column=\"caption\",\n",
    "            max_samples=25000,\n",
    "            weight=0.45,\n",
    "        ),\n",
    "    ])\n",
    "    eval_prompts: List[str] = field(default_factory=lambda: [\n",
    "        \"a cozy watercolor cabin in the woods, warm light\",\n",
    "        \"a futuristic city skyline at dusk, cinematic lighting\",\n",
    "        \"a bowl of ramen sketched in pastel art style\",\n",
    "        \"an astronaut riding a horse through neon fog\",\n",
    "        \"a vibrant street market in Marrakech, detailed\",\n",
    "        \"a majestic lion resting on a rock, photorealistic\",\n",
    "        \"a fantasy landscape with floating islands, vivid colors\",\n",
    "        \"a close-up portrait of a cyberpunk character, dramatic lighting\",\n",
    "        \"a serene beach at sunrise, soft pastel colors\",\n",
    "        \"mario hitting a baseball with pikachu cheering in the background\",\n",
    "        \"a steampunk airship flying over a bustling city, intricate details\",\n",
    "        \"a macro photo of dew on a purple flower, ultra sharp\",\n",
    "    ])\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(f\"Configuration loaded. Image size: {config.image_size}px, total datasets: {len(config.dataset_specs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d36d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def prepare_dirs(cfg: TrainingConfig):\n",
    "    for folder in [cfg.output_dir, cfg.checkpoint_dir, cfg.sample_dir, cfg.data_cache_dir]:\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_seed(config.seed)\n",
    "prepare_dirs(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e5e09",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data ingestion helpers\n",
    "Supports:\n",
    "- HuggingFace datasets (multiple subsets) with automatic discovery + column detection\n",
    "- Local folders staged under `data/` via the `imagefolder` loader\n",
    "- Automatic download + caching of open datasets inside `config.data_cache_dir` so the repo holds everything needed for offline training\n",
    "All datasets are normalized to `{ \"image\": PIL.Image, \"caption\": str, \"source\": str }` so the PyTorch dataset wrapper can blend them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b33864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub missing - skip auto dataset discovery.\n",
      "No extra datasets added via HuggingFace search (check config.auto_dataset_search or install huggingface_hub)\n",
      "Loading cached merged dataset from data\\hf_text2pic\\merged_dataset\n",
      "Dataset mix:\n",
      " - lambdalabs/naruto-blip-captions: 1221 samples (weight 0.2), cached in data\\hf_text2pic\\lambdalabs_naruto-blip-captions\n",
      " - poloclub/diffusiondb: 1000 samples (weight 0.3), cached in data\\hf_text2pic\\poloclub_diffusiondb_2m_first_1k\n",
      "Batches/epoch (approx): 277\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DownloadConfig, load_from_disk\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "hf_download_config = DownloadConfig(resume_download=True, max_retries=3)\n",
    "\n",
    "\n",
    "def resolve_cache_dir(spec: DatasetSpec, cfg: TrainingConfig) -> Path:\n",
    "    slug = spec.name.replace('/', '_')\n",
    "    if spec.subset:\n",
    "        slug += f\"_{spec.subset}\"\n",
    "    cache_dir = Path(cfg.data_cache_dir) / slug\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "def infer_columns(dataset, spec: DatasetSpec):\n",
    "    image_col = spec.image_column\n",
    "    caption_col = spec.caption_column\n",
    "    if (image_col or '').lower() == 'auto':\n",
    "        image_col = next((name for name, feature in dataset.features.items() if feature.__class__.__name__.lower() == 'image' or getattr(feature, '_type', '') == 'Image'), None)\n",
    "    if (caption_col or '').lower() == 'auto':\n",
    "        caption_candidates = [\n",
    "            name for name, feature in dataset.features.items()\n",
    "            if getattr(feature, 'dtype', '') == 'string' or feature.__class__.__name__ == 'Value'\n",
    "        ]\n",
    "        caption_col = caption_candidates[0] if caption_candidates else None\n",
    "    if not image_col or not caption_col:\n",
    "        raise ValueError(f\"Unable to auto-detect columns for {spec.name}. Please set image_column/caption_column explicitly.\")\n",
    "    return image_col, caption_col\n",
    "\n",
    "\n",
    "def load_spec_dataset(spec: DatasetSpec, cfg: TrainingConfig):\n",
    "    cache_dir = resolve_cache_dir(spec, cfg)\n",
    "    if spec.type == \"imagefolder\":\n",
    "        if not spec.local_dir:\n",
    "            raise ValueError(f\"local_dir must be set for imagefolder spec {spec.name}\")\n",
    "        ds = load_dataset(\"imagefolder\", data_dir=spec.local_dir, split=spec.split)\n",
    "        return ds, cache_dir\n",
    "\n",
    "    load_kwargs = dict(\n",
    "        split=spec.split,\n",
    "        streaming=spec.streaming,\n",
    "        cache_dir=str(cache_dir),\n",
    "        trust_remote_code=spec.trust_remote_code,\n",
    "    )\n",
    "    if not spec.streaming:\n",
    "        load_kwargs.update(download_config=hf_download_config, keep_in_memory=False)\n",
    "    print(f\"Downloading {spec.name} ({spec.split}) -> {cache_dir}\")\n",
    "    ds = load_dataset(spec.name, spec.subset, **load_kwargs)\n",
    "    if spec.streaming:\n",
    "        max_take = spec.max_samples or 2000\n",
    "        ds = Dataset.from_list(list(ds.take(max_take)))\n",
    "    return ds, cache_dir\n",
    "\n",
    "\n",
    "def auto_discover_datasets(cfg: TrainingConfig):\n",
    "    if not cfg.use_auto_dataset_search or cfg.auto_dataset_limit <= 0:\n",
    "        return []\n",
    "    try:\n",
    "        from huggingface_hub import list_datasets, DatasetFilter\n",
    "    except ImportError:\n",
    "        print(\"huggingface_hub missing - skip auto dataset discovery.\")\n",
    "        return []\n",
    "    hf_filter = DatasetFilter(task_categories=[\"text-to-image\", \"image-to-text\"])\n",
    "    items = list_datasets(\n",
    "        search=cfg.auto_dataset_search,\n",
    "        filter=hf_filter,\n",
    "        limit=cfg.auto_dataset_limit,\n",
    "    )\n",
    "    extra_specs = []\n",
    "    existing_names = {spec.name for spec in cfg.dataset_specs}\n",
    "    for item in items:\n",
    "        if item.id in existing_names:\n",
    "            continue\n",
    "        extra_specs.append(\n",
    "            DatasetSpec(\n",
    "                name=item.id,\n",
    "                image_column=\"auto\",\n",
    "                caption_column=\"auto\",\n",
    "                max_samples=cfg.auto_dataset_max_samples,\n",
    "                weight=0.15,\n",
    "            )\n",
    "        )\n",
    "    return extra_specs\n",
    "\n",
    "\n",
    "auto_specs = auto_discover_datasets(config)\n",
    "if auto_specs:\n",
    "    config.dataset_specs.extend(auto_specs)\n",
    "    print(f\"Auto-added {len(auto_specs)} datasets from HuggingFace search '{config.auto_dataset_search}'. Total now: {len(config.dataset_specs)}\")\n",
    "else:\n",
    "    print(\"No extra datasets added via HuggingFace search (check config.auto_dataset_search or install huggingface_hub)\")\n",
    "\n",
    "\n",
    "def load_and_merge_datasets(cfg: TrainingConfig):\n",
    "    dataset_pieces = []\n",
    "    stats = []\n",
    "    for spec in cfg.dataset_specs:\n",
    "        if spec.weight <= 0:\n",
    "            continue\n",
    "        ds, cache_dir = load_spec_dataset(spec, cfg)\n",
    "        image_col, caption_col = infer_columns(ds, spec)\n",
    "        keep_map = {image_col: \"image\", caption_col: \"caption\"}\n",
    "        missing = [key for key in keep_map if key not in ds.column_names]\n",
    "        if missing:\n",
    "            print(f\"Skipping {spec.name} \u00e2\u20ac\u201d missing columns {missing}\")\n",
    "            continue\n",
    "        drop_cols = [c for c in ds.column_names if c not in keep_map]\n",
    "        if drop_cols:\n",
    "            ds = ds.remove_columns(drop_cols)\n",
    "        ds = ds.rename_columns(keep_map)\n",
    "        if spec.max_samples and not spec.streaming and spec.max_samples < len(ds):\n",
    "            ds = ds.shuffle(seed=cfg.seed).select(range(spec.max_samples))\n",
    "        size = len(ds)\n",
    "        ds = ds.add_column(\"source\", [spec.name] * size)\n",
    "        ds = ds.add_column(\"weight\", [spec.weight] * size)\n",
    "        dataset_pieces.append(ds)\n",
    "        stats.append({\"name\": spec.name, \"samples\": size, \"weight\": spec.weight, \"cache_dir\": str(cache_dir)})\n",
    "    if not dataset_pieces:\n",
    "        raise RuntimeError(\"No datasets were loaded. Please check DatasetSpec entries or auto-discovery settings.\")\n",
    "    merged = concatenate_datasets(dataset_pieces).shuffle(seed=cfg.seed)\n",
    "    print(f\"Total merged samples: {len(merged)}\")\n",
    "    return merged, stats\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.tokenizer_name,\n",
    "    model_max_length=config.max_prompt_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "def build_transforms(cfg: TrainingConfig):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(cfg.image_size, interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(cfg.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "\n",
    "class TextImageDataset(TorchDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, cfg: TrainingConfig):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "        self.transforms = build_transforms(cfg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[int(idx)]\n",
    "        image = sample[\"image\"]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        else:\n",
    "            image = image.convert(\"RGB\")\n",
    "        caption = sample.get(\"caption\") or \"an image\"\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.cfg.max_prompt_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": self.transforms(image),\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"caption\": caption,\n",
    "            \"source\": sample.get(\"source\", \"unknown\"),\n",
    "        }\n",
    "\n",
    "\n",
    "merged_cache_dir = Path(config.data_cache_dir) / \"merged_dataset\"\n",
    "stats_cache_path = merged_cache_dir / \"stats.json\"\n",
    "\n",
    "if merged_cache_dir.exists() and (merged_cache_dir / \"dataset_info.json\").exists():\n",
    "    print(f\"Loading cached merged dataset from {merged_cache_dir}\")\n",
    "    raw_dataset = load_from_disk(str(merged_cache_dir))\n",
    "    if stats_cache_path.exists():\n",
    "        dataset_stats = json.loads(stats_cache_path.read_text())\n",
    "    else:\n",
    "        dataset_stats = []\n",
    "else:\n",
    "    raw_dataset, dataset_stats = load_and_merge_datasets(config)\n",
    "    merged_cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    raw_dataset.save_to_disk(str(merged_cache_dir))\n",
    "    stats_cache_path.write_text(json.dumps(dataset_stats, indent=2))\n",
    "\n",
    "train_dataset = TextImageDataset(raw_dataset, tokenizer, config)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\"Dataset mix:\")\n",
    "for entry in dataset_stats:\n",
    "    print(f\" - {entry['name']}: {entry['samples']} samples (weight {entry['weight']}), cached in {entry['cache_dir']}\")\n",
    "print(f\"Batches/epoch (approx): {len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcdb7e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model and scheduler setup\n",
    "We use a tiny text encoder (BERT-tiny) and a heavily down-scaled `UNet2DConditionModel` from diffusers. This keeps inference memory under 1 GB (fp16 weights about 140 MB plus runtime buffers). Training uses `Accelerate` so you can scale to multi-GPU later without code changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf250b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with configuration:\n",
      "{'auto_dataset_limit': 6,\n",
      " 'auto_dataset_max_samples': 8000,\n",
      " 'auto_dataset_search': 'text-to-image',\n",
      " 'checkpoint_dir': 'checkpoints/text_to_pic',\n",
      " 'data_cache_dir': 'data/hf_text2pic',\n",
      " 'dataset_specs': [{'caption_column': 'text',\n",
      "                    'image_column': 'image',\n",
      "                    'local_dir': None,\n",
      "                    'max_samples': 12000,\n",
      "                    'name': 'lambdalabs/naruto-blip-captions',\n",
      "                    'split': 'train',\n",
      "                    'streaming': False,\n",
      "                    'subset': None,\n",
      "                    'trust_remote_code': False,\n",
      "                    'type': 'huggingface',\n",
      "                    'weight': 0.1},\n",
      "                   {'caption_column': 'prompt',\n",
      "                    'image_column': 'image',\n",
      "                    'local_dir': None,\n",
      "                    'max_samples': 20000,\n",
      "                    'name': 'poloclub/diffusiondb',\n",
      "                    'split': 'train',\n",
      "                    'streaming': True,\n",
      "                    'subset': '2m_first_1k',\n",
      "                    'trust_remote_code': True,\n",
      "                    'type': 'huggingface',\n",
      "                    'weight': 0.45},\n",
      "                   {'caption_column': 'caption',\n",
      "                    'image_column': 'image',\n",
      "                    'local_dir': None,\n",
      "                    'max_samples': 25000,\n",
      "                    'name': 'conceptual_captions',\n",
      "                    'split': 'train',\n",
      "                    'streaming': False,\n",
      "                    'subset': None,\n",
      "                    'trust_remote_code': False,\n",
      "                    'type': 'huggingface',\n",
      "                    'weight': 0.45}],\n",
      " 'eval_every': 4000,\n",
      " 'eval_prompts': ['a cozy watercolor cabin in the woods, warm light',\n",
      "                  'a futuristic city skyline at dusk, cinematic lighting',\n",
      "                  'a bowl of ramen sketched in pastel art style',\n",
      "                  'an astronaut riding a horse through neon fog',\n",
      "                  'a vibrant street market in Marrakech, detailed',\n",
      "                  'a majestic lion resting on a rock, photorealistic',\n",
      "                  'a fantasy landscape with floating islands, vivid colors',\n",
      "                  'a close-up portrait of a cyberpunk character, dramatic '\n",
      "                  'lighting',\n",
      "                  'a serene beach at sunrise, soft pastel colors',\n",
      "                  'mario hitting a baseball with pikachu cheering in the '\n",
      "                  'background',\n",
      "                  'a steampunk airship flying over a bustling city, intricate '\n",
      "                  'details',\n",
      "                  'a macro photo of dew on a purple flower, ultra sharp'],\n",
      " 'gradient_accumulation_steps': 2,\n",
      " 'guidance_scale': 5.5,\n",
      " 'image_size': 128,\n",
      " 'learning_rate': 5e-05,\n",
      " 'lr_warmup_steps': 2000,\n",
      " 'max_grad_norm': 0.8,\n",
      " 'max_prompt_length': 77,\n",
      " 'max_train_steps': 32000,\n",
      " 'mixed_precision': 'fp16',\n",
      " 'noise_steps': 1000,\n",
      " 'num_epochs': 32,\n",
      " 'num_inference_steps': 40,\n",
      " 'num_workers': 0,\n",
      " 'output_dir': 'outputs/text_to_pic',\n",
      " 'project_name': 'text-to-pic',\n",
      " 'sample_dir': 'outputs/text_to_pic/samples',\n",
      " 'save_every': 2000,\n",
      " 'seed': 42,\n",
      " 'text_encoder_name': 'prajjwal1/bert-tiny',\n",
      " 'tokenizer_name': 'prajjwal1/bert-tiny',\n",
      " 'train_batch_size': 8,\n",
      " 'use_auto_dataset_search': True,\n",
      " 'weight_decay': 0.005}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import AutoModel, get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "\n",
    "device = accelerator.device\n",
    "precision_dtype = torch.float16 if accelerator.mixed_precision == \"fp16\" else (\n",
    "    torch.bfloat16 if accelerator.mixed_precision == \"bf16\" else torch.float32\n",
    ")\n",
    "\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    config.text_encoder_name,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=precision_dtype,\n",
    ")\n",
    "text_encoder.requires_grad_(False)\n",
    "text_encoder.to(device=device, dtype=precision_dtype)\n",
    "text_encoder.eval()\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=config.noise_steps,\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    "    prediction_type=\"epsilon\",\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 192),\n",
    "    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\"),\n",
    "    up_block_types=(\"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n",
    "    attention_head_dim=4,\n",
    "    cross_attention_dim=text_encoder.config.hidden_size,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(unet.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=config.max_train_steps,\n",
    ")\n",
    "\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "accelerator.print(\"Training with configuration:\")\n",
    "accelerator.print(pprint(asdict(config)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbcde8",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training utilities\n",
    "Includes checkpoint saving/loading, preview sampling during training, and the main loop. To actually train set `RUN_TRAINING = True` in the cell after the loop definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d83f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid, save_image\n",
    "import time\n",
    "\n",
    "def unwrap_model_safely(model):\n",
    "    try:\n",
    "        return accelerator.unwrap_model(model)\n",
    "    except (ImportError, ModuleNotFoundError, RuntimeError, AttributeError):\n",
    "        return getattr(model, \"module\", model)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_prompts(prompts, model, tokenizer, text_encoder, scheduler, cfg: TrainingConfig, device, guidance_scale=None, num_inference_steps=None):\n",
    "    model = unwrap_model_safely(model)\n",
    "    model.eval()\n",
    "    scheduler = DDPMScheduler.from_config(scheduler.config)\n",
    "    num_inference_steps = num_inference_steps or cfg.num_inference_steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    guidance_scale = guidance_scale or cfg.guidance_scale\n",
    "    batch_size = len(prompts)\n",
    "    latents = torch.randn((batch_size, 3, cfg.image_size, cfg.image_size), device=device, dtype=precision_dtype)\n",
    "\n",
    "    text_inputs = tokenizer(prompts, max_length=cfg.max_prompt_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "    text_embeddings = text_encoder(**text_inputs).last_hidden_state\n",
    "\n",
    "    uncond_inputs = tokenizer([\" \"] * batch_size, max_length=cfg.max_prompt_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    uncond_inputs = {k: v.to(device) for k, v in uncond_inputs.items()}\n",
    "    uncond_embeddings = text_encoder(**uncond_inputs).last_hidden_state\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_model_input = torch.cat([latents, latents], dim=0)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        encoder_hidden_states = torch.cat([uncond_embeddings, text_embeddings], dim=0)\n",
    "        noise_pred = model(latent_model_input, t, encoder_hidden_states=encoder_hidden_states).sample\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    imgs = (latents * 0.5 + 0.5).clamp(0, 1)\n",
    "    return imgs.cpu()\n",
    "\n",
    "\n",
    "def build_checkpoint_state(step, epoch, best_loss, model, optimizer, lr_scheduler, cfg: TrainingConfig):\n",
    "    unwrapped = unwrap_model_safely(model)\n",
    "    return {\n",
    "        \"unet\": unwrapped.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        \"step\": step,\n",
    "        \"epoch\": epoch,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"config\": asdict(cfg),\n",
    "    }\n",
    "\n",
    "\n",
    "def persist_checkpoint(step, epoch, best_loss, model, optimizer, lr_scheduler, cfg: TrainingConfig, tag: str, state=None, announce: bool = True):\n",
    "    accelerator.wait_for_everyone()\n",
    "    if state is None:\n",
    "        state = build_checkpoint_state(step, epoch, best_loss, model, optimizer, lr_scheduler, cfg)\n",
    "    ckpt_dir = Path(cfg.checkpoint_dir)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = ckpt_dir / f\"{tag}.pt\"\n",
    "    accelerator.save(state, ckpt_path)\n",
    "    if announce and accelerator.is_main_process:\n",
    "        accelerator.print(f\"Saved checkpoint '{tag}' to {ckpt_path}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def resume_from_checkpoint(model, optimizer, lr_scheduler, cfg: TrainingConfig, checkpoint_path: str | None = None):\n",
    "    ckpt_path = Path(checkpoint_path) if checkpoint_path else Path(cfg.checkpoint_dir) / \"latest.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        return 0, 0, float(\"inf\"), None\n",
    "    accelerator.print(f\"Resuming from {ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    unwrap_model_safely(model).load_state_dict(state[\"unet\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(state[\"lr_scheduler\"])\n",
    "    return state.get(\"step\", 0), state.get(\"epoch\", 0), state.get(\"best_loss\", float(\"inf\")), ckpt_path\n",
    "\n",
    "\n",
    "def train_loop(resume_from: str | None = None):\n",
    "    total_batch_size = config.train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps\n",
    "    accelerator.print(f\"Starting training. Effective batch size: {total_batch_size}\")\n",
    "\n",
    "    checkpoint_dir = Path(config.checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    global_step = 0\n",
    "    best_loss = float(\"inf\")\n",
    "    start_epoch = 0\n",
    "\n",
    "    resume_target = resume_from\n",
    "    if resume_target is None:\n",
    "        latest_candidate = checkpoint_dir / \"latest.pt\"\n",
    "        if latest_candidate.exists():\n",
    "            resume_target = str(latest_candidate)\n",
    "\n",
    "    if resume_target:\n",
    "        step, epoch, best_loss, used_path = resume_from_checkpoint(unet, optimizer, lr_scheduler, config, resume_target)\n",
    "        if used_path is not None:\n",
    "            global_step = step\n",
    "            start_epoch = epoch\n",
    "            accelerator.print(f\"Resumed state -> epoch {epoch}, step {step}, best_loss {best_loss:.4f}\")\n",
    "        else:\n",
    "            accelerator.print(f\"Requested resume checkpoint '{resume_target}' not found. Starting fresh.\")\n",
    "\n",
    "    batches_per_epoch = len(train_dataloader)\n",
    "    if batches_per_epoch == 0:\n",
    "        accelerator.print(\"Train dataloader returned 0 batches. Check dataset configuration.\")\n",
    "        return\n",
    "    progress_interval = max(1, batches_per_epoch // 20)\n",
    "    report_every_secs = 30\n",
    "    dataloader_wait_threshold = 20\n",
    "\n",
    "    stop_early = False\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        epoch_start = time.perf_counter()\n",
    "        last_report = epoch_start\n",
    "        if accelerator.is_main_process:\n",
    "            accelerator.print(f\"Epoch {epoch + 1}/{config.num_epochs} started ({batches_per_epoch} batches)\")\n",
    "        data_iter = iter(train_dataloader)\n",
    "\n",
    "        for batch_idx in range(batches_per_epoch):\n",
    "            wait_start = time.perf_counter()\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_dataloader)\n",
    "                batch = next(data_iter)\n",
    "            wait_duration = time.perf_counter() - wait_start\n",
    "            if accelerator.is_main_process and wait_duration >= dataloader_wait_threshold:\n",
    "                accelerator.print(\n",
    "                    f\"Epoch {epoch + 1}/{config.num_epochs} batch {batch_idx + 1}: dataloader stalled {wait_duration:.1f}s\"\n",
    "                )\n",
    "\n",
    "            if global_step >= config.max_train_steps:\n",
    "                stop_early = True\n",
    "                break\n",
    "            with accelerator.accumulate(unet):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=precision_dtype)\n",
    "                noise = torch.randn_like(pixel_values)\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps, (pixel_values.shape[0],), device=device\n",
    "                ).long()\n",
    "                noisy_images = noise_scheduler.add_noise(pixel_values, noise, timesteps).to(pixel_values.dtype)\n",
    "\n",
    "                encoder_input = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(**encoder_input).last_hidden_state.to(precision_dtype)\n",
    "                model_pred = unet(noisy_images, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "                loss = F.mse_loss(model_pred.float(), noise.float())\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            step_loss = loss.detach().item()\n",
    "            epoch_loss += step_loss\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "            time_since_last = time.perf_counter() - last_report\n",
    "            should_report = (\n",
    "                accelerator.is_main_process\n",
    "                and (\n",
    "                    (batch_idx + 1) % progress_interval == 0\n",
    "                    or batch_idx == 0\n",
    "                    or (batch_idx + 1) == batches_per_epoch\n",
    "                    or time_since_last >= report_every_secs\n",
    "                )\n",
    "            )\n",
    "            if should_report:\n",
    "                progress = (batch_idx + 1) / batches_per_epoch\n",
    "                elapsed = time.perf_counter() - epoch_start\n",
    "                eta = (elapsed / progress - elapsed) if progress > 0 else 0.0\n",
    "                accelerator.print(\n",
    "                    f\"Epoch {epoch + 1}/{config.num_epochs} [{batch_idx + 1}/{batches_per_epoch} | {progress * 100:.1f}%] \"\n",
    "                    f\"global step {global_step} | loss {step_loss:.4f} | elapsed {elapsed/60:.1f}m ETA {eta/60:.1f}m\"\n",
    "                )\n",
    "                last_report = time.perf_counter()\n",
    "            if accelerator.is_main_process and config.save_every and global_step % config.save_every == 0:\n",
    "                state = persist_checkpoint(global_step, epoch, best_loss, unet, optimizer, lr_scheduler, config, tag=f\"step_{global_step:06d}\")\n",
    "                persist_checkpoint(global_step, epoch, best_loss, unet, optimizer, lr_scheduler, config, tag=\"latest\", state=state, announce=False)\n",
    "\n",
    "            if accelerator.is_main_process and config.eval_every and global_step % config.eval_every == 0:\n",
    "                imgs = sample_prompts(config.eval_prompts, unet, tokenizer, text_encoder, noise_scheduler, config, device)\n",
    "                grid = make_grid(imgs, nrow=min(4, len(imgs)))\n",
    "                out_path = Path(config.sample_dir) / f\"preview_step_{global_step:06d}.png\"\n",
    "                save_image(grid, out_path)\n",
    "                accelerator.print(f\"Saved preview grid to {out_path}\")\n",
    "\n",
    "        if num_batches == 0:\n",
    "            accelerator.print(f\"Epoch {epoch + 1} had no batches; check dataset configuration.\")\n",
    "            continue\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        epoch_time = time.perf_counter() - epoch_start\n",
    "        if accelerator.is_main_process:\n",
    "            state = persist_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer, lr_scheduler, config, tag=f\"epoch_{epoch + 1:03d}\")\n",
    "            persist_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer, lr_scheduler, config, tag=\"latest\", state=state, announce=False)\n",
    "            if avg_epoch_loss < best_loss:\n",
    "                best_loss = avg_epoch_loss\n",
    "                persist_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer, lr_scheduler, config, tag=\"best\", state=state, announce=False)\n",
    "                accelerator.print(f\"New best loss {best_loss:.4f} at epoch {epoch + 1}\")\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{config.num_epochs} complete | avg loss {avg_epoch_loss:.4f} | time {epoch_time/60:.1f}m\")\n",
    "\n",
    "        if stop_early:\n",
    "            break\n",
    "\n",
    "    accelerator.print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f1c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop is defined but not running. Set RUN_TRAINING = True to start training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RUN_TRAINING = False # <- flip to True when ready\n",
    "if RUN_TRAINING:\n",
    "    train_loop()\n",
    "else:\n",
    "    print(\"Training loop is defined but not running. Set RUN_TRAINING = True to start training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b165cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(prompts=None, checkpoint: str = \"best\", num_inference_steps=None, guidance_scale=None, save_name: str | None = None):\n",
    "    ckpt_dir = Path(config.checkpoint_dir)\n",
    "    if checkpoint in {\"best\", \"latest\"}:\n",
    "        ckpt_path = ckpt_dir / f\"{checkpoint}.pt\"\n",
    "    else:\n",
    "        ckpt_path = Path(checkpoint)\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint {ckpt_path} not found\")\n",
    "\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    unwrap_model_safely(unet).load_state_dict(state[\"unet\"])\n",
    "    prompts = prompts or config.eval_prompts\n",
    "\n",
    "    images = sample_prompts(\n",
    "        prompts,\n",
    "        unet,\n",
    "        tokenizer,\n",
    "        text_encoder,\n",
    "        noise_scheduler,\n",
    "        config,\n",
    "        device,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "    )\n",
    "    grid = make_grid(images, nrow=min(4, len(images)))\n",
    "    stamp = save_name or f\"{ckpt_path.stem}_{int(time.time())}\"\n",
    "    out_path = Path(config.sample_dir) / f\"inference_{stamp}.png\"\n",
    "    save_image(grid, out_path)\n",
    "    accelerator.print(f\"Saved inference grid to {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# Example usage (uncomment to run a quick check):\n",
    "# run_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd474fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_41920\\2842765832.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved single-prompt image to outputs\\text_to_pic\\samples\\single_prompts\\prompt_1763121310.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out_path\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mgenerate_prompt_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma retro myspace-style selfie with neon lighting\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mgenerate_prompt_image\u001b[39m\u001b[34m(prompt, checkpoint, guidance_scale, num_inference_steps, save_name)\u001b[39m\n\u001b[32m     32\u001b[39m save_image(image, out_path)\n\u001b[32m     33\u001b[39m accelerator.print(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved single-prompt image to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m display(\u001b[43mTF\u001b[49m.to_pil_image(image))\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out_path\n",
      "\u001b[31mNameError\u001b[39m: name 'TF' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "SINGLE_PROMPT_DIR = Path(config.sample_dir) / \"single_prompts\"\n",
    "SINGLE_PROMPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def generate_prompt_image(prompt: str, checkpoint: str = \"best\", guidance_scale=None, num_inference_steps=None, save_name: str | None = None):\n",
    "    ckpt_dir = Path(config.checkpoint_dir)\n",
    "    if checkpoint in {\"best\", \"latest\"}:\n",
    "        ckpt_path = ckpt_dir / f\"{checkpoint}.pt\"\n",
    "    else:\n",
    "        ckpt_path = Path(checkpoint)\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint {ckpt_path} not found\")\n",
    "\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    unwrap_model_safely(unet).load_state_dict(state[\"unet\"])\n",
    "\n",
    "    images = sample_prompts(\n",
    "        [prompt],\n",
    "        unet,\n",
    "        tokenizer,\n",
    "        text_encoder,\n",
    "        noise_scheduler,\n",
    "        config,\n",
    "        device,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "    )\n",
    "    image = images[0]\n",
    "    save_slug = save_name or f\"prompt_{int(time.time())}\"\n",
    "    out_path = SINGLE_PROMPT_DIR / f\"{save_slug}.png\"\n",
    "    save_image(image, out_path)\n",
    "    accelerator.print(f\"Saved single-prompt image to {out_path}\")\n",
    "    display(TF.to_pil_image(image))\n",
    "    return out_path\n",
    "\n",
    "# Example usage:\n",
    "generate_prompt_image(\"a retro myspace-style selfie with neon lighting\", checkpoint=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\u00ef\u00bb\u00bffrom torchvision.transforms import functional as TF\n",
    "\n",
    "FINETUNE_ROOT = Path(\"finetune\")\n",
    "FINETUNE_CHECKPOINT_DIR = Path(config.checkpoint_dir) / \"finetune\"\n",
    "FINETUNE_SAMPLE_DIR = Path(config.sample_dir) / \"finetune\"\n",
    "FINETUNE_DEPLOY_DIR = Path(config.output_dir) / \"deployment\" / \"finetune\"\n",
    "ALLOWED_FINETUNE_SUFFIXES = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"}\n",
    "\n",
    "class FinetuneFolderDataset(TorchDataset):\n",
    "    def __init__(self, root: Path, tokenizer, cfg: TrainingConfig, default_caption: str = \"nostalgic myspace portrait\"):\n",
    "        root = Path(root)\n",
    "        files = sorted([p for p in root.rglob(\"*\") if p.suffix.lower() in ALLOWED_FINETUNE_SUFFIXES])\n",
    "        if not files:\n",
    "            raise ValueError(f\"No finetune images found in {root}. Add a few JPG/PNG/WebP files.\")\n",
    "        self.files = files\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "        self.default_caption = default_caption\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: TF.center_crop(img, min(img.size))),\n",
    "            transforms.Resize(cfg.image_size, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.transforms(image)\n",
    "        caption = path.stem.replace(\"_\", \" \") or self.default_caption\n",
    "        tokens = tokenizer(\n",
    "            caption,\n",
    "            max_length=self.cfg.max_prompt_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"caption\": caption,\n",
    "            \"path\": str(path),\n",
    "        }\n",
    "\n",
    "def build_finetune_dataloader(root=FINETUNE_ROOT, default_caption=\"nostalgic myspace portrait\"):\n",
    "    dataset = FinetuneFolderDataset(root, tokenizer, config, default_caption=default_caption)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=max(1, config.train_batch_size // 2),\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=len(dataset) > 1,\n",
    "    )\n",
    "    accelerator.print(f\"Finetune dataset -> {len(dataset)} images from {root}\")\n",
    "    return dataset, loader\n",
    "\n",
    "def save_finetune_checkpoint(step, epoch, best_loss, model, optimizer, scheduler, tag: str, state=None):\n",
    "    FINETUNE_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    state_dict = state if state is not None else {\n",
    "        \"unet\": unwrap_model_safely(model).state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": scheduler.state_dict(),\n",
    "        \"step\": step,\n",
    "        \"epoch\": epoch,\n",
    "        \"best_loss\": best_loss,\n",
    "    }\n",
    "    ckpt_path = FINETUNE_CHECKPOINT_DIR / f\"{tag}.pt\"\n",
    "    accelerator.save(state_dict, ckpt_path)\n",
    "    return state_dict, ckpt_path\n",
    "\n",
    "def run_finetune(epochs: int = 6, resume: str | None = None, base_checkpoint: str | None = None):\n",
    "    dataset, dataloader = build_finetune_dataloader()\n",
    "    total_batches = len(dataloader)\n",
    "    if total_batches == 0:\n",
    "        raise ValueError(\"Finetune dataloader has zero batches.\")\n",
    "\n",
    "    FINETUNE_SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    base_candidates = []\n",
    "    if resume:\n",
    "        base_candidates.append(Path(resume))\n",
    "    if base_checkpoint:\n",
    "        base_candidates.append(Path(base_checkpoint))\n",
    "    base_candidates.append(FINETUNE_CHECKPOINT_DIR / \"finetune_best.pt\")\n",
    "    base_candidates.append(Path(config.checkpoint_dir) / \"best.pt\")\n",
    "    base_candidates.append(Path(config.checkpoint_dir) / \"latest.pt\")\n",
    "    ckpt_path = next((p for p in base_candidates if p and p.exists()), None)\n",
    "    if ckpt_path:\n",
    "        state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        unwrap_model_safely(unet).load_state_dict(state[\"unet\"])\n",
    "        accelerator.print(f\"Loaded base weights from {ckpt_path}\")\n",
    "    else:\n",
    "        accelerator.print(\"No base checkpoint found; finetuning current weights.\")\n",
    "\n",
    "    ft_lr = config.learning_rate * 0.25\n",
    "    optimizer_ft = AdamW(unet.parameters(), lr=ft_lr, weight_decay=config.weight_decay)\n",
    "    total_steps = epochs * total_batches\n",
    "    scheduler_ft = get_cosine_schedule_with_warmup(\n",
    "        optimizer_ft,\n",
    "        num_warmup_steps=max(10, total_steps // 20),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    best_loss = float(\"inf\")\n",
    "    best_path = FINETUNE_CHECKPOINT_DIR / \"finetune_best.pt\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start = time.perf_counter()\n",
    "        if accelerator.is_main_process:\n",
    "            accelerator.print(f\"[Finetune] Epoch {epoch + 1}/{epochs} ({total_batches} batches)\")\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=precision_dtype)\n",
    "                noise = torch.randn_like(pixel_values)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (pixel_values.shape[0],), device=device).long()\n",
    "                noisy_images = noise_scheduler.add_noise(pixel_values, noise, timesteps).to(pixel_values.dtype)\n",
    "                encoder_input = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(**encoder_input).last_hidden_state.to(precision_dtype)\n",
    "                model_pred = unet(noisy_images, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "                loss = F.mse_loss(model_pred.float(), noise.float())\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                optimizer_ft.step()\n",
    "                scheduler_ft.step()\n",
    "                optimizer_ft.zero_grad()\n",
    "\n",
    "            step_loss = loss.detach().item()\n",
    "            epoch_loss += step_loss\n",
    "            global_step += 1\n",
    "            if accelerator.is_main_process:\n",
    "                accelerator.print(\n",
    "                    f\"[Finetune] Epoch {epoch + 1}/{epochs} step {global_step}/{total_steps} | batch {batch_idx + 1}/{total_batches} | loss {step_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = epoch_loss / total_batches\n",
    "        state_dict, _ = save_finetune_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer_ft, scheduler_ft, tag=f\"epoch_{epoch + 1:03d}\")\n",
    "        _, latest_path = save_finetune_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer_ft, scheduler_ft, tag=\"latest\", state=state_dict)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            _, best_path = save_finetune_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer_ft, scheduler_ft, tag=\"finetune_best\", state=state_dict)\n",
    "            accelerator.print(f\"[Finetune] New best loss {best_loss:.4f} at epoch {epoch + 1}\")\n",
    "        accelerator.print(f\"[Finetune] Epoch {epoch + 1}/{epochs} avg loss {avg_loss:.4f} | time {(time.perf_counter() - epoch_start)/60:.1f}m\")\n",
    "\n",
    "        imgs = sample_prompts(config.eval_prompts[:4], unet, tokenizer, text_encoder, noise_scheduler, config, device)\n",
    "        grid = make_grid(imgs, nrow=min(4, len(imgs)))\n",
    "        out_path = FINETUNE_SAMPLE_DIR / f\"finetune_epoch_{epoch + 1:03d}.png\"\n",
    "        save_image(grid, out_path)\n",
    "        accelerator.print(f\"[Finetune] Saved preview to {out_path}\")\n",
    "\n",
    "    accelerator.print(f\"Finetune complete. Latest checkpoint: {latest_path}\")\n",
    "    return latest_path, best_path\n",
    "\n",
    "def export_finetune_model(checkpoint: str | None = None):\n",
    "    FINETUNE_DEPLOY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    if checkpoint is None:\n",
    "        checkpoint = FINETUNE_CHECKPOINT_DIR / \"finetune_best.pt\"\n",
    "    ckpt_path = Path(checkpoint)\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint {ckpt_path} not found.\")\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    unwrap_model_safely(unet).load_state_dict(state[\"unet\"])\n",
    "    traceable_cls = globals().get(\"TraceableUNet\")\n",
    "    if traceable_cls is None:\n",
    "        class TraceableUNet(torch.nn.Module):\n",
    "            def __init__(self, unet):\n",
    "                super().__init__()\n",
    "                self.unet = unet\n",
    "\n",
    "            def forward(self, sample, timestep, encoder_hidden_states):\n",
    "                return self.unet(sample, timestep, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "        traceable_cls = TraceableUNet\n",
    "    model_to_export = unwrap_model_safely(unet).to(torch.float32).cpu()\n",
    "    model_to_export.eval()\n",
    "    traceable = traceable_cls(model_to_export)\n",
    "    dtype = next(model_to_export.parameters()).dtype\n",
    "    dummy_latents = torch.randn(1, 3, config.image_size, config.image_size, dtype=dtype)\n",
    "    dummy_timestep = torch.tensor([0], dtype=torch.long)\n",
    "    dummy_hidden = torch.randn(1, config.max_prompt_length, text_encoder.config.hidden_size, dtype=dtype)\n",
    "    with torch.inference_mode(), torch.autocast(\"cpu\", enabled=False):\n",
    "        traced = torch.jit.trace(traceable, (dummy_latents, dummy_timestep, dummy_hidden), strict=False)\n",
    "        traced_path = FINETUNE_DEPLOY_DIR / \"finetunemodel.ts\"\n",
    "        traced.save(traced_path)\n",
    "    accelerator.print(f\"Exported finetune TorchScript model to {traced_path}\")\n",
    "    return traced_path\n",
    "\n",
    "# Example usage:\n",
    "# latest_path, best_path = run_finetune(epochs=6)\n",
    "# export_finetune_model(best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\u00ef\u00bb\u00bfimport copy\n",
    "\n",
    "STRONGMODEL_DATASETS = copy.deepcopy(config.dataset_specs)\n",
    "STRONGMODEL_EVAL_PROMPTS = list(dict.fromkeys(config.eval_prompts + [\n",
    "    \"a detailed charcoal sketch of a cat lounging on a sofa\",\n",
    "    \"a busy city street at noon, photorealistic\",\n",
    "    \"a vibrant watercolor of mountains and rivers\",\n",
    "    \"a child's drawing of a rocket ship, crayon texture\",\n",
    "    \"a close-up of a succulent plant, macro lens\",\n",
    "    \"an 8-bit pixel art wizard casting a spell\",\n",
    "]))\n",
    "\n",
    "STRONGMODEL_CONFIG = TrainingConfig(\n",
    "    project_name=f\"{config.project_name}-strong\",\n",
    "    output_dir=\"outputs/strong_text_to_pic\",\n",
    "    checkpoint_dir=\"checkpoints/strong_text_to_pic\",\n",
    "    sample_dir=\"outputs/strong_text_to_pic/samples\",\n",
    "    data_cache_dir=config.data_cache_dir,\n",
    "    seed=config.seed,\n",
    "    image_size=256,\n",
    "    train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_epochs=100,\n",
    "    max_train_steps=0,\n",
    "    learning_rate=2e-5,\n",
    "    lr_warmup_steps=4000,\n",
    "    weight_decay=1e-2,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    num_workers=0,\n",
    "    max_grad_norm=0.7,\n",
    "    num_inference_steps=60,\n",
    "    guidance_scale=4.5,\n",
    "    save_every=2000,\n",
    "    eval_every=4000,\n",
    "    max_prompt_length=77,\n",
    "    tokenizer_name=\"openai/clip-vit-base-patch32\",\n",
    "    text_encoder_name=\"openai/clip-vit-base-patch32\",\n",
    "    noise_steps=1000,\n",
    "    use_auto_dataset_search=config.use_auto_dataset_search,\n",
    "    auto_dataset_search=config.auto_dataset_search,\n",
    "    auto_dataset_limit=config.auto_dataset_limit,\n",
    "    auto_dataset_max_samples=config.auto_dataset_max_samples,\n",
    "    dataset_specs=STRONGMODEL_DATASETS,\n",
    "    eval_prompts=STRONGMODEL_EVAL_PROMPTS,\n",
    ")\n",
    "\n",
    "STRONGMODEL_CHECKPOINT_DIR = Path(STRONGMODEL_CONFIG.checkpoint_dir)\n",
    "STRONGMODEL_SAMPLE_DIR = Path(STRONGMODEL_CONFIG.sample_dir)\n",
    "STRONGMODEL_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STRONGMODEL_SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def build_strongmodel_components(cfg: TrainingConfig):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n",
    "    text_encoder = AutoModel.from_pretrained(\n",
    "        cfg.text_encoder_name,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=precision_dtype,\n",
    "    )\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder.to(device=device, dtype=precision_dtype)\n",
    "    text_encoder.eval()\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=cfg.noise_steps,\n",
    "        beta_schedule=\"squaredcos_cap_v2\",\n",
    "        prediction_type=\"epsilon\",\n",
    "    )\n",
    "\n",
    "    unet = UNet2DConditionModel(\n",
    "        sample_size=cfg.image_size,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(256, 320, 512, 640),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"CrossAttnDownBlock2D\",\n",
    "            \"CrossAttnDownBlock2D\",\n",
    "            \"CrossAttnDownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"CrossAttnUpBlock2D\",\n",
    "            \"CrossAttnUpBlock2D\",\n",
    "            \"CrossAttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "        attention_head_dim=8,\n",
    "        cross_attention_dim=text_encoder.config.hidden_size,\n",
    "    )\n",
    "    unet.to(device=device, dtype=precision_dtype)\n",
    "    return tokenizer, text_encoder, noise_scheduler, unet\n",
    "\n",
    "\n",
    "def build_strongmodel_dataloader(cfg: TrainingConfig, tokenizer):\n",
    "    if 'raw_dataset' in globals():\n",
    "        source_dataset = raw_dataset\n",
    "    else:\n",
    "        merged_dir = Path(cfg.data_cache_dir) / \"merged_dataset\"\n",
    "        if merged_dir.exists():\n",
    "            source_dataset = load_from_disk(str(merged_dir))\n",
    "        else:\n",
    "            source_dataset, _ = load_and_merge_datasets(cfg)\n",
    "    dataset = TextImageDataset(source_dataset, tokenizer, cfg)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    accelerator.print(f\"[StrongModel] Dataset ready -> {len(dataset)} samples | {len(loader)} batches/epoch\")\n",
    "    return dataset, loader\n",
    "\n",
    "\n",
    "def save_strongmodel_checkpoint(step, epoch, best_loss, model, optimizer, scheduler, cfg: TrainingConfig, tag: str, state=None):\n",
    "    state_dict = state if state is not None else {\n",
    "        \"unet\": unwrap_model_safely(model).state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": scheduler.state_dict(),\n",
    "        \"step\": step,\n",
    "        \"epoch\": epoch,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"config\": asdict(cfg),\n",
    "    }\n",
    "    ckpt_path = STRONGMODEL_CHECKPOINT_DIR / f\"{tag}.pt\"\n",
    "    accelerator.save(state_dict, ckpt_path)\n",
    "    return state_dict, ckpt_path\n",
    "\n",
    "\n",
    "def train_strongmodel(cfg: TrainingConfig = STRONGMODEL_CONFIG, resume: str | None = None):\n",
    "    tokenizer, text_encoder, noise_scheduler, unet = build_strongmodel_components(cfg)\n",
    "    dataset, dataloader = build_strongmodel_dataloader(cfg, tokenizer)\n",
    "\n",
    "    optimizer = AdamW(unet.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "    total_steps = cfg.num_epochs * len(dataloader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=max(cfg.lr_warmup_steps, total_steps // 20),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, dataloader, scheduler = accelerator.prepare(unet, optimizer, dataloader, scheduler)\n",
    "\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    best_loss = float(\"inf\")\n",
    "    resume_path = Path(resume) if resume else STRONGMODEL_CHECKPOINT_DIR / \"latest.pt\"\n",
    "    if resume_path.exists():\n",
    "        state = torch.load(resume_path, map_location=\"cpu\")\n",
    "        if \"unet\" in state:\n",
    "            unwrap_model_safely(unet).load_state_dict(state[\"unet\"])\n",
    "            optimizer.load_state_dict(state.get(\"optimizer\", optimizer.state_dict()))\n",
    "            scheduler.load_state_dict(state.get(\"lr_scheduler\", scheduler.state_dict()))\n",
    "            best_loss = state.get(\"best_loss\", best_loss)\n",
    "            start_epoch = state.get(\"epoch\", 0)\n",
    "            global_step = state.get(\"step\", 0)\n",
    "            accelerator.print(f\"[StrongModel] Resumed from {resume_path} @ epoch {start_epoch}, step {global_step}\")\n",
    "\n",
    "    for epoch in range(start_epoch, cfg.num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start = time.perf_counter()\n",
    "        noise_scheduler.set_timesteps(cfg.noise_steps, device=device)\n",
    "        for batch_idx, batch in enumerate(dataloader, start=1):\n",
    "            with accelerator.accumulate(unet):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=precision_dtype)\n",
    "                noise = torch.randn_like(pixel_values)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (pixel_values.shape[0],), device=device).long()\n",
    "                noisy_images = noise_scheduler.add_noise(pixel_values, noise, timesteps).to(pixel_values.dtype)\n",
    "\n",
    "                encoder_input = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(**encoder_input).last_hidden_state.to(precision_dtype)\n",
    "\n",
    "                model_pred = unet(noisy_images, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "                loss = F.mse_loss(model_pred.float(), noise.float())\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "            if accelerator.is_main_process and (batch_idx % 50 == 0 or batch_idx == len(dataloader)):\n",
    "                accelerator.print(\n",
    "                    f\"[StrongModel] Epoch {epoch + 1}/{cfg.num_epochs} batch {batch_idx}/{len(dataloader)} | step {global_step}/{total_steps} | loss {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        state_dict, latest_path = save_strongmodel_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer, scheduler, cfg, tag=\"latest\")\n",
    "        save_strongmodel_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer, scheduler, cfg, tag=f\"epoch_{epoch + 1:03d}\", state=state_dict)\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            save_strongmodel_checkpoint(global_step, epoch + 1, best_loss, unet, optimizer, scheduler, cfg, tag=\"best\", state=state_dict)\n",
    "            accelerator.print(f\"[StrongModel] New best loss {best_loss:.4f} at epoch {epoch + 1}\")\n",
    "\n",
    "        imgs = sample_prompts(\n",
    "            cfg.eval_prompts[:8],\n",
    "            unet,\n",
    "            tokenizer,\n",
    "            text_encoder,\n",
    "            noise_scheduler,\n",
    "            cfg,\n",
    "            device,\n",
    "            guidance_scale=cfg.guidance_scale,\n",
    "            num_inference_steps=cfg.num_inference_steps,\n",
    "        )\n",
    "        grid = make_grid(imgs, nrow=min(4, len(imgs)))\n",
    "        preview_path = STRONGMODEL_SAMPLE_DIR / f\"strong_epoch_{epoch + 1:03d}.png\"\n",
    "        save_image(grid, preview_path)\n",
    "        accelerator.print(f\"[StrongModel] Epoch {epoch + 1} complete | avg loss {avg_epoch_loss:.4f} | preview -> {preview_path}\")\n",
    "\n",
    "    accelerator.print(\"[StrongModel] Training finished. Best checkpoint stored as 'best.pt'.\")\n",
    "    return latest_path, STRONGMODEL_CHECKPOINT_DIR / \"best.pt\"\n",
    "\n",
    "# Example usage:\n",
    "# latest_strong, best_strong = train_strongmodel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfd957",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Sampling and qualitative checks\n",
    "Run inference with the latest checkpoint (or current weights) to eyeball quality and verify memory usage. Adjust prompts to cover the domains you care about before pushing live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_checkpoint(path: str):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    unwrap_model_safely(unet).load_state_dict(state[\"unet\"])\n",
    "    accelerator.print(f\"Loaded checkpoint {path}\")\n",
    "    return state\n",
    "\n",
    "ckpt_dir = Path(config.checkpoint_dir)\n",
    "preferred = [ckpt_dir / \"best.pt\", ckpt_dir / \"latest.pt\"]\n",
    "ckpt_path = next((p for p in preferred if p.exists()), None)\n",
    "if ckpt_path is None:\n",
    "    accelerator.print(\"No checkpoints found yet - sampling with current weights.\")\n",
    "else:\n",
    "    _state = load_checkpoint(str(ckpt_path))\n",
    "    accelerator.print(f\"Sampling with weights from {ckpt_path.stem}\")\n",
    "\n",
    "preview = sample_prompts(\n",
    "    config.eval_prompts,\n",
    "    unet,\n",
    "    tokenizer,\n",
    "    text_encoder,\n",
    "    noise_scheduler,\n",
    "    config,\n",
    "    device,\n",
    ")\n",
    "preview_path = Path(config.sample_dir) / \"latest_preview.png\"\n",
    "save_image(make_grid(preview, nrow=min(4, len(preview))), preview_path)\n",
    "accelerator.print(f\"Saved preview to {preview_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TraceableUNet(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "\n",
    "    def forward(self, sample, timestep, encoder_hidden_states):\n",
    "        return self.unet(sample, timestep, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "export_dir = Path(config.output_dir) / \"deployment\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_model = unwrap_model_safely(unet).to(torch.float16).cpu()\n",
    "base_model.eval()\n",
    "traceable = TraceableUNet(base_model)\n",
    "model_dtype = next(base_model.parameters()).dtype\n",
    "\n",
    "dummy_latents = torch.randn(1, 3, config.image_size, config.image_size, dtype=model_dtype)\n",
    "dummy_timestep = torch.tensor([0], dtype=torch.long)\n",
    "dummy_encoder_hidden_states = torch.randn(1, config.max_prompt_length, text_encoder.config.hidden_size, dtype=model_dtype)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    traced = torch.jit.trace(traceable, (dummy_latents, dummy_timestep, dummy_encoder_hidden_states), strict=False)\n",
    "    traced.save(export_dir / \"tiny_text2img_unet_fp16.ts\")\n",
    "\n",
    "print(f\"Exported TorchScript UNet to {export_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384405d",
   "metadata": {},
   "source": [
    "\n",
    "### Web runtime tips\n",
    "- Convert TorchScript to ONNX + WebNN/WebGPU via onnxruntime-web if you need portable WASM/WebGPU inference.\n",
    "- Keep tokenizer + text encoder on the server (ship embeddings only) to cut client RAM by roughly 60 percent.\n",
    "- Use num_inference_steps between 16 and 24 plus guidance_scale around 3.5 for fast single-image renders.\n",
    "- Quantize to int8 or int4 with torch.ao.quantization or bitsandbytes before exporting if you must stay well under 1 GB.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}