<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CMPUT 328 Assignment 7: Autoencoders and Generative Models - CMPUT 328</title>

    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            startup: {
                pageReady: () => {
                    return MathJax.startup.defaultPageReady().then(() => {
                        console.log('MathJax loaded successfully');
                    });
                }
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Courier New', Consolas, Monaco, monospace;
            background-color: #000000;
            color: #ffffff;
            line-height: 1.6;
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }

        h1, h2, h3, h4, h5, h6 {
            text-align: left;
            margin: 30px 0 15px 0;
            font-weight: bold;
            letter-spacing: 1px;
        }

        h1 {
            font-size: 2.5em;
            border-bottom: 3px solid #ffffff;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }

        h2 {
            font-size: 2em;
            border-bottom: 2px solid #ffffff;
            padding-bottom: 8px;
            margin-top: 50px;
        }

        h3 {
            font-size: 1.5em;
            border-left: 5px solid #ffffff;
            padding-left: 15px;
        }

        h4 {
            font-size: 1.2em;
            text-decoration: underline;
        }

        p {
            margin: 15px 0;
            text-align: left;
        }

        ul, ol {
            margin: 15px 0;
            padding-left: 40px;
            text-align: left;
        }

        li {
            margin: 8px 0;
        }

        code {
            background-color: #1a1a1a;
            color: #ffffff;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', Consolas, Monaco, monospace;
            border: 1px solid #333333;
        }

        pre {
            background-color: #1a1a1a;
            color: #ffffff;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid #333333;
            text-align: left;
        }

        pre code {
            background: none;
            border: none;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: #0a0a0a;
        }

        th, td {
            border: 1px solid #ffffff;
            padding: 12px;
            text-align: left;
        }

        th {
            background-color: #1a1a1a;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: #0f0f0f;
        }

        .toc {
            background-color: #1a1a1a;
            border: 2px solid #ffffff;
            padding: 20px;
            margin: 30px 0;
        }

        .toc h2 {
            margin-top: 0;
            border: none;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 20px;
        }

        .toc a {
            color: #ffffff;
            text-decoration: none;
            border-bottom: 1px dotted #ffffff;
        }

        .toc a:hover {
            background-color: #ffffff;
            color: #000000;
        }

        .section {
            margin: 40px 0;
            padding: 20px 0;
        }

        .highlight {
            background-color: #1a1a1a;
            border-left: 5px solid #ffffff;
            padding: 15px;
            margin: 20px 0;
        }

        .formula {
            text-align: center;
            font-size: 1.2em;
            margin: 20px 0;
            padding: 15px;
            background-color: #1a1a1a;
            border: 1px solid #ffffff;
        }

        hr {
            border: none;
            border-top: 1px solid #ffffff;
            margin: 40px 0;
        }

        a {
            color: #ffffff;
            text-decoration: underline;
        }

        a:hover {
            background-color: #ffffff;
            color: #000000;
        }

        .header {
            text-align: center;
            margin-bottom: 50px;
            padding: 30px;
            border: 3px solid #ffffff;
            background-color: #0a0a0a;
        }

        .header h1 {
            border: none;
            margin: 0;
        }

        .header p {
            font-size: 1.2em;
            margin-top: 10px;
            text-align: center;
        }

        .action-buttons {
            display: flex;
            gap: 20px;
            margin: 50px 0;
            justify-content: center;
            flex-wrap: wrap;
        }

        .action-button {
            background-color: #000000;
            color: #ffffff;
            border: 2px solid #ffffff;
            padding: 15px 25px;
            text-transform: uppercase;
            letter-spacing: 1px;
            font-size: 1em;
            cursor: pointer;
            min-width: 220px;
            text-align: center;
        }

        .action-button:hover {
            background-color: #ffffff;
            color: #000000;
        }

        .action-button:active {
            transform: translateY(1px);
        }

        @media print {
            .action-buttons {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>CMPUT 328 Assignment 7: Autoencoders and Generative Models</h1>
        <p>CMPUT 328 - Visual Recognition - Assignment 7</p>
        <p>Comprehensive Study Guide</p>
    </div>

    <div class="toc">
        <h2>Table of Contents</h2>
        <ul>
            <li><a href="#section-1">Comprehensive Study Guide</a></li>
            <li><a href="#section-2">Table of Contents</a></li>
            <li><a href="#section-3">1. Introduction to Autoencoders</a></li>
            <li><a href="#section-4">2. Representation Learning</a></li>
            <li><a href="#section-5">3. Autoencoder Architecture</a></li>
            <li><a href="#section-6">4. Dimensionality Reduction</a></li>
            <li><a href="#section-7">5. Convolutional Autoencoders</a></li>
            <li><a href="#section-8">6. Training Strategies</a></li>
            <li><a href="#section-9">7. Semi-Supervised Learning with Autoencoders</a></li>
            <li><a href="#section-10">8. Denoising Autoencoders</a></li>
            <li><a href="#section-11">9. Probability Primer for Generative Models</a></li>
            <li><a href="#section-12">10. Gaussian Distributions</a></li>
            <li><a href="#section-13">11. Conditional Density Functions</a></li>
            <li><a href="#section-14">12. Introduction to Image Generation</a></li>
            <li><a href="#section-15">13. Variational Autoencoders (VAE)</a></li>
            <li><a href="#section-16">14. KL-Divergence</a></li>
            <li><a href="#section-17">15. Vector Quantized VAE (VQ-VAE)</a></li>
            <li><a href="#section-18">16. Implementation Guide</a></li>
            <li><a href="#section-19">17. Common Pitfalls and Solutions</a></li>
            <li><a href="#section-20">Summary</a></li>
            <li><a href="#section-21">References</a></li>
        </ul>
    </div>


    <div class="section" id="section-3">
        <h2>1. Introduction to Autoencoders</h2>
        <h3>What is an Autoencoder?</h3>
        <p>An <strong>autoencoder</strong> is a neural network designed to learn efficient representations of data in an unsupervised manner. It consists of two main parts:</p>
        <ul>
            <li><strong>Encoder</strong>: Compresses input data into a lower-dimensional latent representation</li>
            <li><strong>Decoder</strong>: Reconstructs the original input from the latent representation</li>
        </ul>
        <ul>
            <li><strong>Key Objective</strong>: Learn to reconstruct input $x$ such that $\hat{x} \approx x$</li>
        </ul>
        <h3>Why Use Autoencoders?</h3>
        <ul>
            <li><strong>Unsupervised Learning</strong>: Learn from unlabeled data</li>
            <li><strong>Dimensionality Reduction</strong>: Compress high-dimensional data</li>
            <li><strong>Feature Learning</strong>: Extract meaningful representations</li>
            <li><strong>Denoising</strong>: Remove noise from corrupted data</li>
            <li><strong>Generation</strong>: Create new data samples (with VAE)</li>
            <li><strong>Pre-training</strong>: Initialize networks for downstream tasks</li>
        </ul>
        <h3>Historical Context</h3>
        <ul>
            <li><strong>1980s</strong>: Basic autoencoder concept introduced</li>
            <li><strong>2006</strong>: Deep autoencoders with layer-wise pre-training (Hinton & Salakhutdinov)</li>
            <li><strong>2013</strong>: Variational Autoencoders (Kingma & Welling)</li>
            <li><strong>2017</strong>: VQ-VAE for discrete latent spaces</li>
            <li><strong>Modern</strong>: Foundation for diffusion models and large-scale generation</li>
        </ul>
    </div>

    <div class="section" id="section-4">
        <h2>2. Representation Learning</h2>
        <h3>What is Representation Learning?</h3>
        <ul>
            <li><strong>Representation learning</strong> is the automatic discovery of representations needed for feature detection or classification from raw data.</li>
        </ul>
        <h3>Why Learn Representations?</h3>
        <p>Traditional approach:
1. Hand-craft features (edges, textures, SIFT, HOG)
2. Train classifier on features</p>
        <p>Autoencoder approach:
1. <strong>Learn features automatically</strong> from unlabeled data
2. Use learned features for downstream tasks</p>
        <h3>Benefits</h3>
        <ul>
            <li><strong>No manual feature engineering</strong></li>
            <li><strong>Captures data structure</strong> automatically</li>
            <li><strong>Transferable</strong> to multiple tasks</li>
            <li><strong>Scales</strong> to large datasets</li>
        </ul>
        <h3>Example: MNIST</h3>
        <p>For MNIST digits (784-dimensional):
- Raw pixels: 28×28 = 784 features
- Autoencoder: Learn 32-dimensional representation
- <strong>Result</strong>: 32 features capture all essential information</p>
        <p>---</p>
    </div>

    <div class="section" id="section-5">
        <h2>3. Autoencoder Architecture</h2>
        <h3>Basic Architecture</h3>
        <pre><code>Input → Encoder → Latent Code → Decoder → Reconstruction
  x   →    φ    →      z      →    ψ    →      x̂</code></pre>
        <h3>Mathematical Formulation</h3>
        <ul>
            <li><strong>Encoder</strong>: $z = f_\phi(x)$ where $z \in \mathbb{R}^d$ and $d < \text{input\_dim}$</li>
        </ul>
        <ul>
            <li><strong>Decoder</strong>: $\hat{x} = g_\psi(z)$</li>
        </ul>
        <ul>
            <li><strong>Loss Function</strong>: Reconstruction error</li>
        </ul>
        <h3>Architecture Types</h3>
        <ul>
            <li><strong>1. Vanilla Autoencoder</strong></li>
            <li>Fully connected layers</li>
            <li>Simple architecture</li>
            <li>Good for structured data</li>
        </ul>
        <ul>
            <li><strong>2. Convolutional Autoencoder</strong></li>
            <li>Conv layers in encoder</li>
            <li>Transposed conv in decoder</li>
            <li>Ideal for images</li>
        </ul>
        <ul>
            <li><strong>3. Sparse Autoencoder</strong></li>
            <li>Adds sparsity constraint on $z$</li>
            <li>Forces learning of sparse representations</li>
        </ul>
        <ul>
            <li><strong>4. Contractive Autoencoder</strong></li>
            <li>Penalizes derivative of hidden layer</li>
            <li>Makes representations robust to small input changes</li>
        </ul>
        <h3>Example: MNIST Autoencoder</h3>
        <p>class VanillaAutoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=32):
        super().__init__()</p>
        <p># Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )</p>
        <p># Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()  # Output in [0, 1]
        )</p>
        <p>def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z
```</p>
        <h3>Common Dimensions</h3>
        <p>For MNIST (28×28 = 784):
- Input: 784
- Hidden layers: 512 → 256 → 128
- Latent: 32 or 64
- Mirror architecture for decoder</p>
        <ul>
            <li><strong>Compression ratio</strong>: 784 / 32 = 24.5× compression!</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-6">
        <h2>4. Dimensionality Reduction</h2>
        <h3>Autoencoders vs PCA</h3>
        <ul>
            <li><strong>PCA (Principal Component Analysis)</strong>:</li>
            <li>Linear transformation</li>
            <li>Finds orthogonal directions of maximum variance</li>
            <li>$z = W^T x$ where $W$ are principal components</li>
        </ul>
        <ul>
            <li><strong>Autoencoder</strong>:</li>
            <li><strong>Non-linear</strong> transformation</li>
            <li>Can learn complex manifolds</li>
            <li>More expressive than PCA</li>
        </ul>
        <h3>When Autoencoders Match PCA</h3>
        <p>If encoder and decoder are linear with no activation:
$$z = W_e x$$
$$\hat{x} = W_d z$$</p>
        <p>With MSE loss, this learns PCA subspace!</p>
        <h3>When Autoencoders Excel</h3>
        <p>With <strong>non-linear activations</strong> (ReLU, tanh), autoencoders can:
- Learn curved manifolds
- Capture complex relationships
- Achieve better reconstruction</p>
        <h3>Visualization Example</h3>
        <p>For 3D data compressed to 2D:</p>
        <ul>
            <li><strong>PCA</strong>: Projects onto best-fitting plane</li>
            <li><strong>Autoencoder</strong>: Can learn curved 2D surface embedded in 3D</li>
        </ul>
        <h3>Implementation</h3>
        <p># PCA
pca = PCA(n_components=32)
z_pca = pca.fit_transform(X_train)
X_recon_pca = pca.inverse_transform(z_pca)</p>
        <p># Autoencoder
model = VanillaAutoencoder(input_dim=784, latent_dim=32)
X_recon_ae, z_ae = model(torch.tensor(X_train))</p>
        <p># Compare reconstruction error
mse_pca = np.mean((X_train - X_recon_pca) ** 2)
mse_ae = np.mean((X_train - X_recon_ae.detach().numpy()) ** 2)</p>
        <p>print(f"PCA MSE: {mse_pca:.4f}")
print(f"Autoencoder MSE: {mse_ae:.4f}")
```</p>
        <p>---</p>
    </div>

    <div class="section" id="section-7">
        <h2>5. Convolutional Autoencoders</h2>
        <h3>Why Convolutional?</h3>
        <p>For image data:
- <strong>Preserve spatial structure</strong>
- <strong>Parameter efficiency</strong> (weight sharing)
- <strong>Translation invariance</strong>
- <strong>Better features</strong> for visual data</p>
        <h3>Architecture</h3>
        <ul>
            <li><strong>Encoder</strong>: Conv + Pooling (downsample)</li>
            <li><strong>Decoder</strong>: Transposed Conv (upsample)</li>
        </ul>
        <h3>Example Architecture</h3>
        <p># Encoder: 28x28 -> 14x14 -> 7x7
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 28->14
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 14->7
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=7)  # 7->1
        )</p>
        <p># Decoder: 1x1 -> 7x7 -> 14x14 -> 28x28
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, kernel_size=7),  # 1->7
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 7->14
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),   # 14->28
            nn.Sigmoid()
        )</p>
        <p>def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z
```</p>
        <h3>Output Size Calculation</h3>
        <ul>
            <li><strong>Convolution</strong>:</li>
        </ul>
        <ul>
            <li><strong>Transposed Convolution</strong>:</li>
        </ul>
        <h3>Skip Connections (U-Net Style)</h3>
        <p>For better reconstruction, add skip connections:</p>
        <p># Encoder
        self.enc1 = nn.Conv2d(1, 64, 3, padding=1)
        self.enc2 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2)</p>
        <p># Decoder
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = nn.Conv2d(128, 64, 3, padding=1)  # 128 = 64 + 64 (skip)
        self.dec2 = nn.Conv2d(64, 1, 3, padding=1)</p>
        <p>def forward(self, x):
        # Encoder
        e1 = F.relu(self.enc1(x))
        e2 = F.relu(self.enc2(self.pool(e1)))</p>
        <p># Decoder with skip connection
        d1 = self.up1(e2)
        d1 = torch.cat([d1, e1], dim=1)  # Skip connection
        d1 = F.relu(self.dec1(d1))
        out = torch.sigmoid(self.dec2(d1))</p>
        <p>return out, e2
```</p>
        <p>---</p>
    </div>

    <div class="section" id="section-8">
        <h2>6. Training Strategies</h2>
        <h3>Standard Training</h3>
        <p>for epoch in range(num_epochs):
    for batch in dataloader:
        x = batch.to(device)</p>
        <p># Forward pass
        x_recon, z = model(x)
        loss = criterion(x_recon, x)</p>
        <p># Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```</p>
        <h3>Phased Training (Greedy Layer-wise)</h3>
        <ul>
            <li><strong>Idea</strong>: Train layers one at a time (Hinton & Salakhutdinov, 2006)</li>
        </ul>
        <ul>
            <li><strong>Phase 1</strong>: Train first encoder-decoder pair</li>
        </ul>
        <ul>
            <li><strong>Phase 2</strong>: Freeze E1, train second pair</li>
        </ul>
        <ul>
            <li><strong>Benefits</strong>:</li>
            <li>Easier optimization</li>
            <li>Better initialization</li>
            <li><strong>Historical</strong>: Important before batch normalization and good initialization</li>
        </ul>
        <ul>
            <li><strong>Modern Usage</strong>: Less common now, but still useful for very deep networks</li>
        </ul>
        <h3>Loss Functions</h3>
        <ul>
            <li><strong>1. Mean Squared Error (L2)</strong>:</li>
        </ul>
        <ul>
            <li>Best for continuous values</li>
            <li>Penalizes large errors heavily</li>
        </ul>
        <ul>
            <li><strong>2. Binary Cross-Entropy</strong>:</li>
        </ul>
        <ul>
            <li>Best for binary or normalized images</li>
            <li>Use with sigmoid output</li>
        </ul>
        <ul>
            <li><strong>3. L1 Loss</strong>:</li>
        </ul>
        <ul>
            <li>More robust to outliers</li>
            <li>Encourages sparsity</li>
        </ul>
        <h3>Regularization</h3>
        <ul>
            <li><strong>Sparsity Constraint</strong>:</li>
        </ul>
        <p>Forces sparse latent codes.</p>
        <ul>
            <li><strong>Weight Decay</strong>:</li>
        </ul>
        <p>Prevents overfitting.</p>
        <h3>Training Tips</h3>
        <ul>
            <li><strong>Normalize inputs</strong> to [0, 1] or [-1, 1]</li>
            <li><strong>Use batch normalization</strong> in encoder/decoder</li>
            <li><strong>Learning rate</strong>: Start with 1e-3, reduce if loss plateaus</li>
            <li><strong>Monitor reconstruction quality</strong> visually</li>
            <li><strong>Check for mode collapse</strong>: Ensure varied reconstructions</li>
            <li><strong>Gradient clipping</strong> if training unstable</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-9">
        <h2>7. Semi-Supervised Learning with Autoencoders</h2>
        <h3>The Setup</h3>
        <ul>
            <li><strong>Problem</strong>: Have lots of unlabeled data, few labeled samples</li>
        </ul>
        <ul>
            <li><strong>Solution</strong>: Use autoencoder for pre-training</li>
        </ul>
        <h3>Two-Stage Approach</h3>
        <ul>
            <li><strong>Stage 1: Unsupervised Pre-training</strong></li>
        </ul>
        <ul>
            <li><strong>Stage 2: Supervised Fine-tuning</strong></li>
        </ul>
        <p>def forward(self, x):
        z = self.encoder(x)
        return self.classifier(z)</p>
        <p># Fine-tune on labeled data
model = Classifier(autoencoder.encoder, num_classes=10)
train_classifier(model, labeled_data)
```</p>
        <h3>Why This Works</h3>
        <ul>
            <li><strong>Feature learning</strong>: Encoder learns general features from all data</li>
            <li><strong>Regularization</strong>: Pre-training prevents overfitting on small labeled set</li>
            <li><strong>Transfer learning</strong>: Features transfer to classification task</li>
        </ul>
        <h3>Joint Training Alternative</h3>
        <p>Train both objectives simultaneously:
$$L = L_{\text{recon}} + \alpha L_{\text{classification}}$$</p>
        <p>def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        logits = self.classifier(z)
        return x_recon, logits</p>
        <p># Training loop
for batch_x, batch_y in dataloader:
    x_recon, logits = model(batch_x)</p>
        <p>loss_recon = F.mse_loss(x_recon, batch_x)
    loss_class = F.cross_entropy(logits, batch_y) if batch_y is not None else 0</p>
        <p>loss = loss_recon + alpha * loss_class
    loss.backward()
```</p>
        <h3>Expected Results</h3>
        <p>With 100 labeled MNIST samples:
- <strong>No pre-training</strong>: ~70% accuracy
- <strong>With autoencoder pre-training</strong>: ~85% accuracy</p>
        <ul>
            <li><strong>Key insight</strong>: Unlabeled data provides structure of data manifold</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-10">
        <h2>8. Denoising Autoencoders</h2>
        <h3>What is a Denoising Autoencoder (DAE)?</h3>
        <ul>
            <li><strong>Idea</strong>: Train autoencoder to <strong>reconstruct clean input from corrupted version</strong></li>
        </ul>
        <p>$$\tilde{x} = \text{Corrupt}(x)$$
$$\hat{x} = \text{Decoder}(\text{Encoder}(\tilde{x}))$$
$$L = \|x - \hat{x}\|^2$$</p>
        <h3>Types of Corruption</h3>
        <ul>
            <li><strong>1. Gaussian Noise</strong>:</li>
        </ul>
        <ul>
            <li><strong>2. Masking Noise (Dropout)</strong>:</li>
        </ul>
        <ul>
            <li><strong>3. Salt-and-Pepper Noise</strong>:</li>
        </ul>
        <ul>
            <li><strong>4. Blur</strong>:</li>
        </ul>
        <h3>Why Denoising?</h3>
        <ul>
            <li><strong>Robustness</strong>: Learn features invariant to noise</li>
            <li><strong>Better representations</strong>: Forces model to capture data structure</li>
            <li><strong>Prevents memorization</strong>: Can't just copy input</li>
            <li><strong>Implicit regularization</strong>: Acts as data augmentation</li>
        </ul>
        <h3>Implementation</h3>
        <p>def add_noise(self, x, noise_type='gaussian', noise_level=0.3):
        if noise_type == 'gaussian':
            noise = torch.randn_like(x) * noise_level
            return torch.clamp(x + noise, 0, 1)</p>
        <p>elif noise_type == 'masking':
            mask = torch.rand_like(x) > noise_level
            return x * mask.float()</p>
        <p>return x</p>
        <p>def forward(self, x, add_noise=True):
        if add_noise and self.training:
            x_noisy = self.add_noise(x)
        else:
            x_noisy = x</p>
        <p>z = self.encoder(x_noisy)
        x_recon = self.decoder(z)
        return x_recon, z</p>
        <p># Training
model = DenoisingAutoencoder()
for batch in dataloader:
    x = batch.to(device)
    x_recon, z = model(x, add_noise=True)</p>
        <p># Loss: reconstruct CLEAN input from NOISY input
    loss = F.mse_loss(x_recon, x)  # Note: compare to original x, not noisy!</p>
        <p>loss.backward()
    optimizer.step()
```</p>
        <h3>Noise Level Selection</h3>
        <ul>
            <li><strong>Too low</strong> (σ < 0.1): Model might still memorize</li>
            <li><strong>Optimal</strong> (σ = 0.2-0.4): Good balance</li>
            <li><strong>Too high</strong> (σ > 0.5): Task becomes too difficult</li>
        </ul>
        <ul>
            <li><strong>Tip</strong>: Start with 0.3, adjust based on reconstruction quality</li>
        </ul>
        <h3>Applications</h3>
        <ul>
            <li><strong>Image denoising</strong>: Remove noise from photos</li>
            <li><strong>Inpainting</strong>: Fill in missing regions</li>
            <li><strong>Super-resolution</strong>: Upscale low-res images</li>
            <li><strong>Artifact removal</strong>: Remove compression artifacts</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-11">
        <h2>9. Probability Primer for Generative Models</h2>
        <h3>Random Variables</h3>
        <ul>
            <li><strong>Discrete Random Variable</strong> $X$:</li>
            <li>Takes countable values: $X \in \{x_1, x_2, \ldots, x_n\}$</li>
            <li>Example: Die roll, $X \in \{1, 2, 3, 4, 5, 6\}$</li>
        </ul>
        <ul>
            <li><strong>Continuous Random Variable</strong> $X$:</li>
            <li>Takes values in continuous range: $X \in \mathbb{R}$ or $X \in [a, b]$</li>
            <li>Example: Height, temperature, pixel intensity</li>
        </ul>
        <h3>Probability Mass Function (PMF)</h3>
        <p>For discrete $X$:
$$P(X = x_i) = p_i$$</p>
        <ul>
            <li><strong>Properties</strong>:</li>
            <li>$0 \leq p_i \leq 1$ for all $i$</li>
            <li>$\sum_{i=1}^n p_i = 1$</li>
        </ul>
        <ul>
            <li><strong>Example</strong>: Fair die</li>
        </ul>
        <h3>Probability Density Function (PDF)</h3>
        <p>For continuous $X$:
$$p(x) = \frac{dP(X \leq x)}{dx}$$</p>
        <ul>
            <li><strong>Properties</strong>:</li>
            <li>$p(x) \geq 0$ for all $x$</li>
            <li>$\int_{-\infty}^{\infty} p(x) \, dx = 1$</li>
        </ul>
        <ul>
            <li><strong>Note</strong>: $p(x)$ can be > 1! It's a density, not a probability.</li>
        </ul>
        <ul>
            <li><strong>Probability of interval</strong>:</li>
        </ul>
        <h3>Expectation</h3>
        <ul>
            <li><strong>Discrete</strong>:</li>
        </ul>
        <ul>
            <li><strong>Continuous</strong>:</li>
        </ul>
        <h3>Variance</h3>
        <p>$$\text{Var}(X) = \mathbb{E}[(X - \mu)^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$</p>
        <p>where $\mu = \mathbb{E}[X]$</p>
        <h3>Multiple Random Variables</h3>
        <ul>
            <li><strong>Joint Distribution</strong>: $p(x, y)$</li>
        </ul>
        <ul>
            <li><strong>Marginal Distribution</strong>:</li>
        </ul>
        <ul>
            <li><strong>Conditional Distribution</strong>:</li>
        </ul>
        <ul>
            <li><strong>Independence</strong>:</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-12">
        <h2>10. Gaussian Distributions</h2>
        <h3>Univariate Gaussian (1D)</h3>
        <p>$$p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$</p>
        <ul>
            <li><strong>Parameters</strong>:</li>
            <li>$\mu$: Mean (center)</li>
            <li>$\sigma^2$: Variance (spread)</li>
            <li>$\sigma$: Standard deviation</li>
        </ul>
        <ul>
            <li><strong>Notation</strong>: $X \sim \mathcal{N}(\mu, \sigma^2)$</li>
        </ul>
        <h3>Properties</h3>
        <ul>
            <li><strong>Symmetric</strong> around $\mu$</li>
            <li><strong>68-95-99.7 rule</strong>:</li>
            <li>68% of data within $\mu \pm \sigma$</li>
            <li>95% within $\mu \pm 2\sigma$</li>
            <li>99.7% within $\mu \pm 3\sigma$</li>
            <li><strong>Maximum at</strong> $x = \mu$</li>
        </ul>
        <h3>Standard Normal</h3>
        <p>Special case: $\mu = 0$, $\sigma^2 = 1$
$$p(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)$$</p>
        <ul>
            <li><strong>Notation</strong>: $X \sim \mathcal{N}(0, 1)$</li>
        </ul>
        <h3>Multivariate Gaussian (Vector)</h3>
        <p>For $\mathbf{x} \in \mathbb{R}^d$:
$$p(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)$$</p>
        <ul>
            <li><strong>Parameters</strong>:</li>
            <li>$\boldsymbol{\mu} \in \mathbb{R}^d$: Mean vector</li>
            <li>$\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$: Covariance matrix</li>
        </ul>
        <ul>
            <li><strong>Notation</strong>: $\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$</li>
        </ul>
        <h3>Diagonal Covariance</h3>
        <p>Often assume <strong>diagonal</strong> covariance:
$$\boldsymbol{\Sigma} = \text{diag}(\sigma_1^2, \sigma_2^2, \ldots, \sigma_d^2)$$</p>
        <p>Then:
$$p(\mathbf{x}) = \prod_{i=1}^d \frac{1}{\sqrt{2\pi\sigma_i^2}} \exp\left(-\frac{(x_i-\mu_i)^2}{2\sigma_i^2}\right)$$</p>
        <ul>
            <li><strong>Simplification</strong>: Variables are independent!</li>
        </ul>
        <h3>Sampling from Gaussian</h3>
        <ul>
            <li><strong>1D</strong>:</li>
        </ul>
        <p>mu, sigma = 0, 1
samples = np.random.normal(mu, sigma, size=1000)
```</p>
        <ul>
            <li><strong>Multivariate</strong>:</li>
        </ul>
        <h3>Why Gaussian for VAE?</h3>
        <ul>
            <li><strong>Mathematically tractable</strong>: Easy to compute KL-divergence</li>
            <li><strong>Central Limit Theorem</strong>: Many distributions converge to Gaussian</li>
            <li><strong>Universal</strong>: Can approximate many distributions</li>
            <li><strong>Differentiable</strong>: Easy to optimize</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-13">
        <h2>11. Conditional Density Functions</h2>
        <h3>Definition</h3>
        <ul>
            <li><strong>Conditional density</strong> $p(x \mid y)$: Probability of $x$ given $y$</li>
        </ul>
        <ul>
            <li><strong>Bayes' Rule</strong>:</li>
        </ul>
        <p>where:
- $p(x \mid y)$: Posterior
- $p(y \mid x)$: Likelihood
- $p(x)$: Prior
- $p(y)$: Evidence (marginal)</p>
        <h3>Example: Image Generation</h3>
        <ul>
            <li><strong>Goal</strong>: Generate image $x$ conditioned on class $y$</li>
        </ul>
        <p>$$p(x \mid y) = \text{"probability of image } x \text{ given it's class } y\text{"}$$</p>
        <p>Examples:
- $p(\text{image} \mid y=\text{"cat"})$: Distribution of cat images
- $p(\text{image} \mid y=\text{"dog"})$: Distribution of dog images</p>
        <h3>Conditional Gaussian</h3>
        <p>$$p(x \mid y) = \mathcal{N}(x \mid \mu(y), \sigma^2(y))$$</p>
        <p>Mean and variance depend on condition $y$!</p>
        <ul>
            <li><strong>Example</strong>:</li>
            <li>$p(\text{height} \mid \text{gender=male}) = \mathcal{N}(175, 100)$</li>
            <li>$p(\text{height} \mid \text{gender=female}) = \mathcal{N}(162, 90)$</li>
        </ul>
        <h3>Chain Rule of Probability</h3>
        <p>For multiple variables:
$$p(x_1, x_2, \ldots, x_n) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) \cdots p(x_n \mid x_1, \ldots, x_{n-1})$$</p>
        <ul>
            <li><strong>Application</strong>: Autoregressive models (PixelCNN, Transformers)</li>
        </ul>
        <h3>In VAE Context</h3>
        <ul>
            <li><strong>Decoder is conditional density</strong>:</li>
        </ul>
        <p>"Probability of image $x$ given latent code $z$"</p>
        <ul>
            <li><strong>Encoder is approximate posterior</strong>:</li>
        </ul>
        <p>"Probability of latent $z$ given image $x$"</p>
        <h3>Conditional Generation Example</h3>
        <p># Encoder: p(z | x, y)
        self.encoder = nn.Sequential(...)</p>
        <p># Decoder: p(x | z, y)
        self.decoder = nn.Sequential(...)</p>
        <p># Embed class label
        self.class_embed = nn.Embedding(num_classes, latent_dim)</p>
        <p>def encode(self, x, y):
        h = self.encoder(x)
        y_embed = self.class_embed(y)
        h = h + y_embed  # Condition on class
        mu, logvar = torch.chunk(h, 2, dim=1)
        return mu, logvar</p>
        <p>def decode(self, z, y):
        y_embed = self.class_embed(y)
        z_cond = z + y_embed  # Condition on class
        return self.decoder(z_cond)</p>
        <p>def forward(self, x, y):
        mu, logvar = self.encode(x, y)
        z = reparameterize(mu, logvar)
        x_recon = self.decode(z, y)
        return x_recon, mu, logvar
```</p>
        <p>---</p>
    </div>

    <div class="section" id="section-14">
        <h2>12. Introduction to Image Generation</h2>
        <h3>Types of Generative Models</h3>
        <ul>
            <li><strong>1. Generative Adversarial Networks (GANs)</strong></li>
            <li>Two networks: Generator $G$ and Discriminator $D$</li>
            <li>Adversarial training: $\min_G \max_D V(D, G)$</li>
            <li><strong>Pros</strong>: High-quality images</li>
            <li><strong>Cons</strong>: Training instability, mode collapse</li>
        </ul>
        <ul>
            <li><strong>2. Variational Autoencoders (VAE)</strong></li>
            <li>Probabilistic encoder and decoder</li>
            <li>Optimize ELBO (Evidence Lower Bound)</li>
            <li><strong>Pros</strong>: Stable training, principled framework</li>
            <li><strong>Cons</strong>: Blurry images (in basic form)</li>
        </ul>
        <ul>
            <li><strong>3. Flow-Based Models (Normalizing Flows)</strong></li>
            <li>Invertible transformations</li>
            <li>Exact likelihood computation</li>
            <li><strong>Pros</strong>: Exact inference, stable training</li>
            <li><strong>Cons</strong>: Architecture constraints</li>
        </ul>
        <ul>
            <li><strong>4. Diffusion Models</strong></li>
            <li>Learn to reverse noise process</li>
            <li>State-of-the-art quality (Stable Diffusion, DALL-E)</li>
            <li><strong>Pros</strong>: Best quality, stable training</li>
            <li><strong>Cons</strong>: Slow sampling</li>
        </ul>
        <h3>Why VAE for This Course?</h3>
        <ul>
            <li><strong>Principled</strong>: Based on probabilistic inference</li>
            <li><strong>Stable</strong>: Easier to train than GANs</li>
            <li><strong>Interpretable</strong>: Latent space has structure</li>
            <li><strong>Versatile</strong>: Can add conditions, disentangle factors</li>
            <li><strong>Foundation</strong>: Understanding VAE helps with other models</li>
        </ul>
        <h3>Generative vs Discriminative Models</h3>
        <ul>
            <li><strong>Discriminative</strong>: $p(y \mid x)$</li>
            <li>Learns decision boundary</li>
            <li>Examples: Classifiers (ResNet, VGGNet)</li>
        </ul>
        <ul>
            <li><strong>Generative</strong>: $p(x)$ or $p(x, y)$</li>
            <li>Learns data distribution</li>
            <li>Can generate new samples</li>
            <li>Examples: VAE, GAN, Diffusion</li>
        </ul>
        <h3>The Generation Pipeline</h3>
        <pre><code>1. Sample latent code: z ~ N(0, I)
2. Decode to image: x = Decoder(z)
3. Result: New image x</code></pre>
        <ul>
            <li><strong>Key insight</strong>: Latent space is <strong>continuous</strong>, so can:</li>
            <li><strong>Interpolate</strong>: $z_{\text{new}} = 0.5 z_1 + 0.5 z_2$</li>
            <li><strong>Arithmetic</strong>: $z_{\text{smile}} = z_{\text{smiling}} - z_{\text{neutral}}$</li>
            <li><strong>Explore</strong>: Walk through latent space</li>
        </ul>
        <h3>Evaluation Metrics</h3>
        <ul>
            <li><strong>1. Reconstruction Quality</strong> (for autoencoders):</li>
            <li>MSE, PSNR, SSIM</li>
        </ul>
        <ul>
            <li><strong>2. Generation Quality</strong>:</li>
            <li><strong>FID (Fréchet Inception Distance)</strong>: Measures distribution similarity</li>
            <li><strong>IS (Inception Score)</strong>: Measures quality and diversity</li>
            <li><strong>Human evaluation</strong>: Still the gold standard</li>
        </ul>
        <ul>
            <li><strong>3. Latent Space Quality</strong>:</li>
            <li><strong>Disentanglement</strong>: Independent factors</li>
            <li><strong>Smoothness</strong>: Nearby points → similar images</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-15">
        <h2>13. Variational Autoencoders (VAE)</h2>
        <h3>Motivation</h3>
        <ul>
            <li><strong>Problem with standard autoencoders</strong>:</li>
            <li>Latent space can have "holes"</li>
            <li>Can't generate new samples (only reconstruct)</li>
            <li>No structure in latent space</li>
        </ul>
        <ul>
            <li><strong>VAE solution</strong>:</li>
            <li>Learn <strong>probabilistic</strong> encoding</li>
            <li>Force latent distribution to match prior (usually $\mathcal{N}(0, I)$)</li>
            <li>Can sample from prior to generate!</li>
        </ul>
        <h3>Probabilistic Formulation</h3>
        <ul>
            <li><strong>Goal</strong>: Model data distribution $p(x)$</li>
        </ul>
        <ul>
            <li><strong>Latent variable model</strong>:</li>
        </ul>
        <p>where:
- $p(z) = \mathcal{N}(0, I)$: Prior (simple Gaussian)
- $p_\theta(x \mid z)$: Decoder (learned)</p>
        <ul>
            <li><strong>Problem</strong>: Computing $p(x)$ requires intractable integral!</li>
        </ul>
        <h3>Variational Inference</h3>
        <ul>
            <li><strong>Idea</strong>: Approximate intractable posterior $p(z \mid x)$ with tractable $q_\phi(z \mid x)$</li>
        </ul>
        <ul>
            <li><strong>Encoder</strong>: $q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \sigma_\phi^2(x))$</li>
        </ul>
        <p>Learn $\mu_\phi$ and $\sigma_\phi$ with neural networks!</p>
        <h3>Evidence Lower Bound (ELBO)</h3>
        <ul>
            <li><strong>Derivation</strong>:</li>
        </ul>
        <p>$$= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)} \frac{q_\phi(z|x)}{p(z|x)}\right]$$</p>
        <p>$$= \underbrace{\mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right]}_{\text{ELBO}} + \underbrace{D_{KL}(q_\phi(z|x) \| p(z|x))}_{\geq 0}$$</p>
        <p>Since $D_{KL} \geq 0$:
$$\log p(x) \geq \text{ELBO}$$</p>
        <ul>
            <li><strong>ELBO (Evidence Lower Bound)</strong>:</li>
        </ul>
        <h3>VAE Loss Function</h3>
        <p>$$\text{Loss} = -\mathcal{L} = \underbrace{-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} + \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{KL Divergence}}$$</p>
        <ul>
            <li><strong>Two terms</strong>:</li>
        </ul>
        <ul>
            <li><strong>Reconstruction</strong>: How well can we reconstruct $x$ from $z$?</li>
            <li>In practice: $\|x - \hat{x}\|^2$ (MSE)</li>
        </ul>
        <ul>
            <li><strong>KL Divergence</strong>: How different is $q(z|x)$ from prior $p(z)$?</li>
            <li>Forces latent distribution to be $\mathcal{N}(0, I)$</li>
        </ul>
        <h3>The Reparameterization Trick</h3>
        <ul>
            <li><strong>Problem</strong>: Can't backprop through sampling $z \sim q_\phi(z|x)$</li>
        </ul>
        <ul>
            <li><strong>Solution</strong>: Reparameterize!</li>
        </ul>
        <ul>
            <li><strong>Original</strong>:</li>
        </ul>
        <ul>
            <li><strong>Reparameterized</strong>:</li>
        </ul>
        <p>where $\odot$ is element-wise multiplication.</p>
        <ul>
            <li><strong>Now</strong>: Randomness is in $\epsilon$ (fixed distribution), and $z$ is deterministic function of $\mu, \sigma, \epsilon$</li>
        </ul>
        <ul>
            <li><strong>Backprop works!</strong></li>
        </ul>
        <h3>VAE Architecture</h3>
        <p># Encoder: x -> mu, logvar
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 400),
            nn.ReLU(),
            nn.Linear(400, 400),
            nn.ReLU()
        )
        self.fc_mu = nn.Linear(400, latent_dim)
        self.fc_logvar = nn.Linear(400, latent_dim)</p>
        <p># Decoder: z -> x
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 400),
            nn.ReLU(),
            nn.Linear(400, 400),
            nn.ReLU(),
            nn.Linear(400, input_dim),
            nn.Sigmoid()
        )</p>
        <p>def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar</p>
        <p>def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 <em> logvar)  # sigma = exp(0.5 </em> log(sigma^2))
        eps = torch.randn_like(std)    # Sample epsilon ~ N(0, 1)
        z = mu + eps <em> std             # z = mu + sigma </em> epsilon
        return z</p>
        <p>def decode(self, z):
        return self.decoder(z)</p>
        <p>def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z)
        return x_recon, mu, logvar
```</p>
        <h3>VAE Loss Implementation</h3>
        <p># KL divergence: D_KL(N(mu, sigma) || N(0, 1))
    # Formula: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())</p>
        <p>return recon_loss + kl_loss</p>
        <p># Training loop
model = VAE()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)</p>
        <p>for epoch in range(num_epochs):
    for batch in dataloader:
        x = batch.to(device)</p>
        <p>x_recon, mu, logvar = model(x)
        loss = vae_loss(x, x_recon, mu, logvar)</p>
        <p>optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```</p>
        <h3>Why VAE Works for Generation</h3>
        <ul>
            <li><strong>Structured latent space</strong>: KL term forces $q(z|x) \approx \mathcal{N}(0, I)$</li>
            <li><strong>Smooth</strong>: Nearby $z$ → similar $x$</li>
            <li><strong>Complete</strong>: No holes in latent space</li>
            <li><strong>Sampling</strong>: $z \sim \mathcal{N}(0, I)$, then $x = \text{Decoder}(z)$</li>
        </ul>
        <h3>Generating New Images</h3>
        <p># Decode
with torch.no_grad():
    samples = model.decode(z)</p>
        <p># Visualize
import matplotlib.pyplot as plt
fig, axes = plt.subplots(8, 8, figsize=(8, 8))
for i, ax in enumerate(axes.flat):
    ax.imshow(samples[i].cpu().reshape(28, 28), cmap='gray')
    ax.axis('off')
plt.show()
```</p>
        <h3>Interpolation in Latent Space</h3>
        <p># Interpolate
alphas = torch.linspace(0, 1, 10)
for alpha in alphas:
    z_interp = alpha <em> mu1 + (1 - alpha) </em> mu2
    img_interp = model.decode(z_interp)
    # Display img_interp
```</p>
        <p>---</p>
    </div>

    <div class="section" id="section-16">
        <h2>14. KL-Divergence</h2>
        <h3>Definition</h3>
        <ul>
            <li><strong>KL-divergence</strong> (Kullback-Leibler divergence) measures how one probability distribution $Q$ differs from another $P$:</li>
        </ul>
        <ul>
            <li><strong>Discrete</strong>:</li>
        </ul>
        <ul>
            <li><strong>Continuous</strong>:</li>
        </ul>
        <h3>Properties</h3>
        <ul>
            <li><strong>Non-negative</strong>: $D_{KL}(P \| Q) \geq 0$</li>
            <li><strong>Zero iff equal</strong>: $D_{KL}(P \| Q) = 0$ iff $P = Q$</li>
            <li><strong>Asymmetric</strong>: $D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$ in general</li>
            <li><strong>Not a metric</strong>: Doesn't satisfy triangle inequality</li>
        </ul>
        <h3>Intuition</h3>
        <p>$D_{KL}(P \| Q)$ = "How much information is lost when $Q$ is used to approximate $P$"</p>
        <ul>
            <li>Large $D_{KL}$: $P$ and $Q$ are very different</li>
            <li>Small $D_{KL}$: $P$ and $Q$ are similar</li>
        </ul>
        <h3>Example: Two Gaussians</h3>
        <p>$$P = \mathcal{N}(\mu_1, \sigma_1^2), \quad Q = \mathcal{N}(\mu_2, \sigma_2^2)$$</p>
        <p>$$D_{KL}(P \| Q) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$</p>
        <ul>
            <li><strong>Special case</strong>: $Q = \mathcal{N}(0, 1)$</li>
        </ul>
        <p>$$D_{KL}(\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, 1)) = \frac{1}{2}\left(\mu^2 + \sigma^2 - \log \sigma^2 - 1\right)$$</p>
        <h3>Multivariate Case</h3>
        <p>For $P = \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ and $Q = \mathcal{N}(0, I)$:</p>
        <p>$$D_{KL}(P \| Q) = \frac{1}{2}\left(\text{tr}(\boldsymbol{\Sigma}) + \boldsymbol{\mu}^T\boldsymbol{\mu} - k - \log \det(\boldsymbol{\Sigma})\right)$$</p>
        <p>where $k$ is dimensionality.</p>
        <ul>
            <li><strong>With diagonal covariance</strong> $\boldsymbol{\Sigma} = \text{diag}(\sigma_1^2, \ldots, \sigma_k^2)$:</li>
        </ul>
        <p>$$D_{KL}(P \| Q) = \frac{1}{2} \sum_{i=1}^k \left(\mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1\right)$$</p>
        <h3>Implementation</h3>
        <p>Args:
        mu: Mean of shape (batch, latent_dim)
        logvar: Log variance of shape (batch, latent_dim)</p>
        <p>Returns:
        KL divergence summed over latent dimensions
    """
    # Formula: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
    return kl.mean()  # Average over batch
```</p>
        <h3>Why This Formula?</h3>
        <p>Start with definition:
$$D_{KL}(q(z|x) \| p(z)) = \mathbb{E}_{q(z|x)}\left[\log \frac{q(z|x)}{p(z)}\right]$$</p>
        <p>For $q(z|x) = \mathcal{N}(\mu, \sigma^2)$ and $p(z) = \mathcal{N}(0, 1)$:</p>
        <p>$$D_{KL} = \mathbb{E}\left[\log q(z|x) - \log p(z)\right]$$</p>
        <p>$$= \mathbb{E}\left[-\frac{(z-\mu)^2}{2\sigma^2} - \frac{1}{2}\log(2\pi\sigma^2) + \frac{z^2}{2} + \frac{1}{2}\log(2\pi)\right]$$</p>
        <p>After simplification (taking expectation over $z \sim \mathcal{N}(\mu, \sigma^2)$):</p>
        <p>$$D_{KL} = \frac{1}{2}\left(\mu^2 + \sigma^2 - \log \sigma^2 - 1\right)$$</p>
        <p>In code, we use $\log \sigma^2$ (logvar) directly:</p>
        <p>$$D_{KL} = \frac{1}{2}\left(\mu^2 + \exp(\text{logvar}) - \text{logvar} - 1\right)$$</p>
        <h3>Role in VAE</h3>
        <ul>
            <li><strong>KL term regularizes latent space</strong>:</li>
        </ul>
        <ul>
            <li><strong>Forces $q(z|x) \approx p(z)$</strong>: Latent codes follow standard normal</li>
            <li><strong>Prevents overfitting</strong>: Can't just memorize training data</li>
            <li><strong>Enables sampling</strong>: Can sample $z \sim \mathcal{N}(0, I)$ to generate</li>
        </ul>
        <ul>
            <li><strong>Trade-off</strong>:</li>
            <li><strong>High KL weight</strong>: Blurry reconstructions (too much regularization)</li>
            <li><strong>Low KL weight</strong>: Better reconstructions but worse generation (latent space not normalized)</li>
        </ul>
        <h3>β-VAE</h3>
        <p>Adjust KL weight:
$$\text{Loss} = \text{Recon} + \beta \cdot D_{KL}$$</p>
        <ul>
            <li>$\beta < 1$: Better reconstruction</li>
            <li>$\beta > 1$: More disentangled latent space</li>
            <li>$\beta = 1$: Standard VAE</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-17">
        <h2>15. Vector Quantized VAE (VQ-VAE)</h2>
        <h3>Motivation</h3>
        <ul>
            <li><strong>Problem with standard VAE</strong>:</li>
            <li>Continuous latent space</li>
            <li>Blurry reconstructions (due to sampling)</li>
            <li>Difficult to model complex distributions</li>
        </ul>
        <ul>
            <li><strong>VQ-VAE solution</strong>:</li>
            <li><strong>Discrete</strong> latent space</li>
            <li>Learn a <strong>codebook</strong> of embeddings</li>
            <li>Quantize encoder output to nearest codebook entry</li>
        </ul>
        <h3>Architecture</h3>
        <pre><code>Input → Encoder → Continuous z_e → Quantization → Discrete z_q → Decoder → Output</code></pre>
        <ul>
            <li><strong>Key difference</strong>: Replace sampling with <strong>vector quantization</strong></li>
        </ul>
        <h3>The Codebook</h3>
        <ul>
            <li><strong>Codebook</strong> $\mathcal{C} = \{e_1, e_2, \ldots, e_K\}$ where $e_k \in \mathbb{R}^D$</li>
        </ul>
        <ul>
            <li>$K$: Number of codebook entries (e.g., 512)</li>
            <li>$D$: Embedding dimension (e.g., 64)</li>
        </ul>
        <ul>
            <li><strong>Quantization</strong>: Map encoder output $z_e$ to nearest codebook entry $e_k$</li>
        </ul>
        <p>$$z_q = e_k \text{ where } k = \arg\min_j \|z_e - e_j\|_2$$</p>
        <h3>VQ-VAE Loss</h3>
        <p>Three components:</p>
        <p>$$\text{Loss} = \underbrace{\|x - \hat{x}\|^2}_{\text{Reconstruction}} + \underbrace{\|\text{sg}[z_e] - e_k\|_2^2}_{\text{Codebook}} + \underbrace{\beta \|z_e - \text{sg}[e_k]\|_2^2}_{\text{Commitment}}$$</p>
        <p>where $\text{sg}[\cdot]$ means stop gradient (no backprop through this term).</p>
        <ul>
            <li><strong>1. Reconstruction Loss</strong>: Standard autoencoder loss</li>
        </ul>
        <ul>
            <li><strong>2. Codebook Loss</strong>: Moves codebook entries $e_k$ towards encoder outputs $z_e$</li>
            <li>Updates codebook</li>
            <li>Gradient only flows to $e_k$</li>
        </ul>
        <ul>
            <li><strong>3. Commitment Loss</strong>: Forces encoder to commit to a codebook entry</li>
            <li>Prevents encoder from growing arbitrarily</li>
            <li>Gradient only flows to encoder</li>
        </ul>
        <h3>Straight-Through Estimator</h3>
        <ul>
            <li><strong>Problem</strong>: Quantization is non-differentiable!</li>
        </ul>
        <p>$$z_q = \arg\min_k \|z_e - e_k\|_2$$</p>
        <ul>
            <li><strong>Solution</strong>: In backward pass, copy gradient from decoder to encoder</li>
        </ul>
        <p>$$\frac{\partial \text{Loss}}{\partial z_e} = \frac{\partial \text{Loss}}{\partial z_q}$$</p>
        <p>"Pretend" quantization is identity in backward pass.</p>
        <h3>Implementation</h3>
        <p># Codebook
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)</p>
        <p>def forward(self, z_e):
        # z_e: (batch, embedding_dim, H, W)</p>
        <p># Flatten spatial dimensions
        z_e_flat = z_e.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)</p>
        <p># Calculate distances to codebook entries
        distances = (z_e_flat.pow(2).sum(1, keepdim=True)
                    + self.embedding.weight.pow(2).sum(1)
                    - 2 * z_e_flat @ self.embedding.weight.t())</p>
        <p># Find nearest codebook entry
        encoding_indices = torch.argmin(distances, dim=1)</p>
        <p># Quantize
        z_q_flat = self.embedding(encoding_indices)
        z_q = z_q_flat.view_as(z_e.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)</p>
        <p># Compute losses
        codebook_loss = F.mse_loss(z_q.detach(), z_e)
        commitment_loss = F.mse_loss(z_q, z_e.detach())</p>
        <p># Straight-through estimator
        z_q = z_e + (z_q - z_e).detach()</p>
        <p>loss = codebook_loss + self.commitment_cost * commitment_loss</p>
        <p>return z_q, loss, encoding_indices</p>
        <p>class VQVAE(nn.Module):
    def __init__(self, num_embeddings=512, embedding_dim=64):
        super().__init__()</p>
        <p>self.encoder = Encoder(embedding_dim)  # Output: (batch, 64, H/4, W/4)
        self.quantizer = VectorQuantizer(num_embeddings, embedding_dim)
        self.decoder = Decoder(embedding_dim)  # Input: (batch, 64, H/4, W/4)</p>
        <p>def forward(self, x):
        z_e = self.encoder(x)
        z_q, vq_loss, indices = self.quantizer(z_e)
        x_recon = self.decoder(z_q)</p>
        <p>return x_recon, vq_loss, indices
```</p>
        <h3>Training</h3>
        <p>for batch in dataloader:
    x = batch.to(device)</p>
        <p>x_recon, vq_loss, _ = model(x)</p>
        <p>recon_loss = F.mse_loss(x_recon, x)
    loss = recon_loss + vq_loss</p>
        <p>optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```</p>
        <h3>Advantages of VQ-VAE</h3>
        <ul>
            <li><strong>Discrete latent space</strong>: No posterior collapse</li>
            <li><strong>Sharper reconstructions</strong>: No sampling blur</li>
            <li><strong>Better for autoregressive models</strong>: Can model $p(z)$ with PixelCNN</li>
            <li><strong>Interpretable</strong>: Codebook entries are learned visual concepts</li>
        </ul>
        <h3>Disadvantages</h3>
        <ul>
            <li><strong>More complex</strong>: Three loss terms, straight-through estimator</li>
            <li><strong>Codebook collapse</strong>: Some entries may never be used</li>
            <li><strong>Hyperparameter sensitive</strong>: Need to tune commitment cost, codebook size</li>
        </ul>
        <h3>Codebook Collapse Prevention</h3>
        <ul>
            <li><strong>Problem</strong>: Some codebook entries never get used</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>EMA (Exponential Moving Average) updates</strong>:</li>
        </ul>
        <ul>
            <li><strong>Restart unused entries</strong>:</li>
        </ul>
        <ul>
            <li><strong>Increase commitment cost</strong>: Force encoder to use all entries</li>
        </ul>
        <h3>Applications</h3>
        <ul>
            <li><strong>High-quality image generation</strong>: DALL-E uses VQ-VAE</li>
            <li><strong>Audio generation</strong>: Jukebox (OpenAI)</li>
            <li><strong>Video compression</strong>: Discrete latent codes</li>
            <li><strong>Hierarchical models</strong>: VQ-VAE-2 with multiple levels</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-18">
        <h2>16. Implementation Guide</h2>
        <h3>Complete VAE Training Script</h3>
        <p># Hyperparameters
BATCH_SIZE = 128
LATENT_DIM = 20
LEARNING_RATE = 1e-3
NUM_EPOCHS = 50
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</p>
        <p># Data loading
transform = transforms.Compose([
    transforms.ToTensor(),
])</p>
        <p>train_dataset = datasets.MNIST('data/', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('data/', train=False, transform=transform)</p>
        <p>train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)</p>
        <p># Model
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()</p>
        <p># Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)</p>
        <p># Decoder
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, hidden_dim)
        self.fc5 = nn.Linear(hidden_dim, input_dim)</p>
        <p>def encode(self, x):
        h = F.relu(self.fc1(x))
        h = F.relu(self.fc2(h))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar</p>
        <p>def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std</p>
        <p>def decode(self, z):
        h = F.relu(self.fc3(z))
        h = F.relu(self.fc4(h))
        return torch.sigmoid(self.fc5(h))</p>
        <p>def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar</p>
        <p># Loss function
def vae_loss(x, x_recon, mu, logvar):
    # Reconstruction loss
    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')</p>
        <p># KL divergence
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())</p>
        <p>return recon_loss + kl_loss</p>
        <p># Training
model = VAE(latent_dim=LATENT_DIM).to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)</p>
        <p>def train(epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(-1, 784).to(DEVICE)</p>
        <p>optimizer.zero_grad()</p>
        <p>x_recon, mu, logvar = model(data)
        loss = vae_loss(data, x_recon, mu, logvar)</p>
        <p>loss.backward()
        train_loss += loss.item()
        optimizer.step()</p>
        <p>if batch_idx % 100 == 0:
            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '
                  f'Loss: {loss.item() / len(data):.4f}')</p>
        <p>avg_loss = train_loss / len(train_loader.dataset)
    print(f'====> Epoch {epoch} Average loss: {avg_loss:.4f}')</p>
        <p>def test(epoch):
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for data, _ in test_loader:
            data = data.view(-1, 784).to(DEVICE)
            x_recon, mu, logvar = model(data)
            loss = vae_loss(data, x_recon, mu, logvar)
            test_loss += loss.item()</p>
        <p>avg_loss = test_loss / len(test_loader.dataset)
    print(f'====> Test set loss: {avg_loss:.4f}')</p>
        <p># Main training loop
for epoch in range(1, NUM_EPOCHS + 1):
    train(epoch)
    test(epoch)</p>
        <p># Save samples
    if epoch % 5 == 0:
        with torch.no_grad():
            z = torch.randn(64, LATENT_DIM).to(DEVICE)
            samples = model.decode(z).cpu().view(64, 1, 28, 28)</p>
        <p>fig, axes = plt.subplots(8, 8, figsize=(8, 8))
            for i, ax in enumerate(axes.flat):
                ax.imshow(samples[i, 0], cmap='gray')
                ax.axis('off')
            plt.savefig(f'samples_epoch_{epoch}.png')
            plt.close()</p>
        <p># Save model
torch.save(model.state_dict(), 'vae_mnist.pth')
```</p>
        <h3>Convolutional VAE</h3>
        <p># Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 4, stride=2, padding=1),  # 28 -> 14
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1), # 14 -> 7
            nn.ReLU(),
            nn.Conv2d(64, 128, 7),  # 7 -> 1
            nn.ReLU()
        )</p>
        <p>self.fc_mu = nn.Linear(128, latent_dim)
        self.fc_logvar = nn.Linear(128, latent_dim)</p>
        <p># Decoder
        self.fc_decode = nn.Linear(latent_dim, 128)</p>
        <p>self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 7),  # 1 -> 7
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 7 -> 14
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),   # 14 -> 28
            nn.Sigmoid()
        )</p>
        <p>def encode(self, x):
        h = self.encoder(x)
        h = h.view(h.size(0), -1)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar</p>
        <p>def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std</p>
        <p>def decode(self, z):
        h = F.relu(self.fc_decode(z))
        h = h.view(-1, 128, 1, 1)
        return self.decoder(h)</p>
        <p>def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```</p>
        <h3>Latent Space Visualization</h3>
        <p>with torch.no_grad():
        for data, label in dataloader:
            data = data.view(-1, 784).to(device)
            mu, _ = model.encode(data)
            latents.append(mu.cpu().numpy())
            labels.append(label.numpy())</p>
        <p>latents = np.concatenate(latents)
    labels = np.concatenate(labels)</p>
        <p># Use t-SNE for visualization if latent_dim > 2
    if latents.shape[1] > 2:
        from sklearn.manifold import TSNE
        latents_2d = TSNE(n_components=2).fit_transform(latents)
    else:
        latents_2d = latents</p>
        <p># Plot
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(latents_2d[:, 0], latents_2d[:, 1], c=labels, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title('Latent Space Visualization')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.show()</p>
        <p>visualize_latent_space(model, test_loader, DEVICE)
```</p>
        <h3>Interpolation</h3>
        <p>with torch.no_grad():
        mu1, _ = model.encode(img1.view(-1, 784).to(device))
        mu2, _ = model.encode(img2.view(-1, 784).to(device))</p>
        <p>interpolations = []
        for alpha in torch.linspace(0, 1, steps):
            z = alpha <em> mu1 + (1 - alpha) </em> mu2
            img = model.decode(z)
            interpolations.append(img.cpu().view(28, 28))</p>
        <p># Plot
        fig, axes = plt.subplots(1, steps, figsize=(15, 2))
        for i, ax in enumerate(axes):
            ax.imshow(interpolations[i], cmap='gray')
            ax.axis('off')
        plt.show()</p>
        <p># Example usage
img1 = test_dataset[0][0]
img2 = test_dataset[1][0]
interpolate(model, img1, img2, steps=10, device=DEVICE)
```</p>
        <p>---</p>
    </div>

    <div class="section" id="section-19">
        <h2>17. Common Pitfalls and Solutions</h2>
        <h3>1. Posterior Collapse in VAE</h3>
        <ul>
            <li><strong>Problem</strong>: KL divergence goes to zero, model ignores latent code</li>
        </ul>
        <ul>
            <li><strong>Symptoms</strong>:</li>
            <li>KL loss ≈ 0</li>
            <li>All samples look the same</li>
            <li>Decoder becomes unconditional</li>
        </ul>
        <ul>
            <li><strong>Causes</strong>:</li>
            <li>Decoder too powerful</li>
            <li>KL weight too high early in training</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. KL Annealing</strong>:</li>
        </ul>
        <p># In training loop
loss = recon_loss + kl_weight(epoch) * kl_loss
```</p>
        <ul>
            <li><strong>B. Free Bits</strong>:</li>
        </ul>
        <p>kl_loss = free_bits_kl(kl_loss, free_bits=2.0)
```</p>
        <ul>
            <li><strong>C. Weaken decoder</strong>:</li>
            <li>Use fewer layers</li>
            <li>Reduce hidden dimensions</li>
            <li>Add dropout</li>
        </ul>
        <h3>2. Blurry Reconstructions</h3>
        <ul>
            <li><strong>Problem</strong>: VAE produces blurry images</li>
        </ul>
        <ul>
            <li><strong>Causes</strong>:</li>
            <li>MSE/BCE loss averages over all possible reconstructions</li>
            <li>Sampling introduces noise</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. Increase latent dimension</strong>:</li>
        </ul>
        <ul>
            <li><strong>B. Use VQ-VAE</strong>: Discrete latents eliminate sampling noise</li>
        </ul>
        <ul>
            <li><strong>C. Perceptual loss</strong>:</li>
        </ul>
        <p>def forward(self, x, x_recon):
        features_x = self.feature_extractor(x)
        features_recon = self.feature_extractor(x_recon)
        return F.mse_loss(features_recon, features_x)</p>
        <p># Use in training
perceptual_loss_fn = PerceptualLoss()
loss = recon_loss + 0.1 * perceptual_loss_fn(x, x_recon) + kl_loss
```</p>
        <h3>3. Mode Collapse</h3>
        <ul>
            <li><strong>Problem</strong>: Model generates limited variety of samples</li>
        </ul>
        <ul>
            <li><strong>Symptoms</strong>:</li>
            <li>All samples look similar</li>
            <li>Doesn't cover full data distribution</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. Increase KL weight (β-VAE)</strong>:</li>
        </ul>
        <ul>
            <li><strong>B. Increase latent dimension</strong></li>
        </ul>
        <ul>
            <li><strong>C. Architectural improvements</strong>:</li>
            <li>Deeper encoder/decoder</li>
            <li>More parameters</li>
        </ul>
        <h3>4. Numerical Instability</h3>
        <ul>
            <li><strong>Problem</strong>: NaN or Inf in loss</li>
        </ul>
        <ul>
            <li><strong>Causes</strong>:</li>
            <li><code>log(0)</code> in KL divergence</li>
            <li>Exponential overflow in <code>exp(logvar)</code></li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. Clamp logvar</strong>:</li>
        </ul>
        <ul>
            <li><strong>B. Use <code>log_softmax</code> for stability</strong>:</li>
        </ul>
        <p># Use
loss = F.binary_cross_entropy_with_logits(logits, x)
```</p>
        <ul>
            <li><strong>C. Gradient clipping</strong>:</li>
        </ul>
        <h3>5. Training Time Issues</h3>
        <ul>
            <li><strong>Problem</strong>: VAE trains slowly</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. Larger batch size</strong>:</li>
        </ul>
        <ul>
            <li><strong>B. Learning rate scheduling</strong>:</li>
        </ul>
        <ul>
            <li><strong>C. Use convolutional architecture</strong>: Much faster than fully connected for images</li>
        </ul>
        <h3>6. Evaluation Mode Errors</h3>
        <ul>
            <li><strong>Problem</strong>: Forgot to set <code>model.eval()</code> during generation</li>
        </ul>
        <ul>
            <li><strong>Issue</strong>: Batch norm and dropout behave differently</li>
        </ul>
        <ul>
            <li><strong>Solution</strong>:</li>
        </ul>
        <h3>7. Codebook Collapse (VQ-VAE)</h3>
        <ul>
            <li><strong>Problem</strong>: Only a few codebook entries are used</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. EMA updates</strong> instead of gradient-based</li>
        </ul>
        <ul>
            <li><strong>B. Monitor codebook usage</strong>:</li>
        </ul>
        <p># In training loop
unique_indices, counts = torch.unique(encoding_indices, return_counts=True)
usage_count[unique_indices] += counts
```</p>
        <ul>
            <li><strong>C. Reset unused entries</strong>:</li>
        </ul>
        <h3>8. Wrong Loss Scale</h3>
        <ul>
            <li><strong>Problem</strong>: Reconstruction and KL losses have very different scales</li>
        </ul>
        <ul>
            <li><strong>Symptoms</strong>:</li>
            <li>One dominates the other</li>
            <li>Poor balance</li>
        </ul>
        <ul>
            <li><strong>Solution</strong>: Normalize losses or adjust weights:</li>
        </ul>
        <p># Or adjust weight
loss = recon_loss + beta * kl_loss
```</p>
        <h3>9. Memory Issues</h3>
        <ul>
            <li><strong>Problem</strong>: Out of memory errors</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. Reduce batch size</strong></li>
        </ul>
        <ul>
            <li><strong>B. Use gradient checkpointing</strong>:</li>
        </ul>
        <p>def forward(self, x):
    mu, logvar = checkpoint(self.encode, x)
    z = self.reparameterize(mu, logvar)
    x_recon = checkpoint(self.decode, z)
    return x_recon, mu, logvar
```</p>
        <ul>
            <li><strong>C. Use mixed precision training</strong>:</li>
        </ul>
        <p>scaler = GradScaler()</p>
        <p>with autocast():
    x_recon, mu, logvar = model(x)
    loss = vae_loss(x, x_recon, mu, logvar)</p>
        <p>scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```</p>
        <h3>10. Overfitting</h3>
        <ul>
            <li><strong>Problem</strong>: Good training loss, poor test performance</li>
        </ul>
        <ul>
            <li><strong>Solutions</strong>:</li>
        </ul>
        <ul>
            <li><strong>A. Data augmentation</strong>:</li>
        </ul>
        <ul>
            <li><strong>B. Dropout in encoder/decoder</strong>:</li>
        </ul>
        <ul>
            <li><strong>C. Weight decay</strong>:</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-20">
        <h2>Summary</h2>
        <h3>Key Takeaways</h3>
        <ul>
            <li><strong>Autoencoders</strong> learn compressed representations through reconstruction</li>
            <li><strong>Denoising</strong> improves robustness and representation quality</li>
            <li><strong>VAE</strong> adds probabilistic structure for generation</li>
            <li><strong>KL divergence</strong> regularizes latent space to match prior</li>
            <li><strong>Reparameterization trick</strong> enables backpropagation through sampling</li>
            <li><strong>VQ-VAE</strong> uses discrete latents for sharper reconstructions</li>
        </ul>
        <h3>Comparison Table</h3>
        <p>| Model | Latent Space | Generation | Reconstruction | Training |
|-------|-------------|------------|----------------|----------|
| Autoencoder | Continuous, unstructured | ❌ No | ✅ Sharp | Easy |
| VAE | Continuous, structured | ✅ Yes | ⚠️ Blurry | Medium |
| VQ-VAE | Discrete | ✅ Yes (with prior) | ✅ Sharp | Hard |</p>
        <h3>When to Use Each</h3>
        <ul>
            <li><strong>Autoencoder</strong>:</li>
            <li>Dimensionality reduction</li>
            <li>Feature learning for classification</li>
            <li>Denoising</li>
            <li>Don't need generation</li>
        </ul>
        <ul>
            <li><strong>VAE</strong>:</li>
            <li>Need to generate new samples</li>
            <li>Want smooth latent space</li>
            <li>Probabilistic modeling</li>
            <li>Conditional generation</li>
        </ul>
        <ul>
            <li><strong>VQ-VAE</strong>:</li>
            <li>Need sharp reconstructions</li>
            <li>Discrete latent space useful (e.g., for autoregressive models)</li>
            <li>Have computational resources for more complex training</li>
        </ul>
        <h3>Expected Performance on MNIST</h3>
        <ul>
            <li><strong>Autoencoder</strong>: Reconstruction MSE ≈ 0.01-0.02</li>
            <li><strong>VAE (latent_dim=20)</strong>: Reconstruction MSE ≈ 0.02-0.04, good generation</li>
            <li><strong>VQ-VAE (512 codes)</strong>: Reconstruction MSE ≈ 0.01-0.02, sharp generation</li>
        </ul>
        <p>---</p>
    </div>

    <div class="section" id="section-21">
        <h2>References</h2>
        <ul>
            <li>Kingma & Welling (2013). Auto-Encoding Variational Bayes. [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)</li>
            <li>van den Oord et al. (2017). Neural Discrete Representation Learning (VQ-VAE). [arXiv:1711.00937](https://arxiv.org/abs/1711.00937)</li>
            <li>Higgins et al. (2017). β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. [ICLR 2017]</li>
            <li>Vincent et al. (2008). Extracting and Composing Robust Features with Denoising Autoencoders.</li>
            <li>Hinton & Salakhutdinov (2006). Reducing the Dimensionality of Data with Neural Networks. Science.</li>
        </ul>
        <p>---</p>
        <p><strong>End of Lesson</strong></p>
        <p>Good luck with Assignment 7!</p>
    </div>


    <hr>
    <div style="text-align: center; padding: 30px 0;">
        <p><strong>End of Lesson</strong></p>
        <p>Good luck with Assignment 7!</p>
        <p style="margin-top: 20px; font-size: 0.9em;">
            Created: November 13, 2025<br>
            Source: Autoencoders (1).pdf (35 pages)<br>
            CMPUT 328 - Visual Recognition
        </p>
    </div>


    <div class="action-buttons">
        <button class="action-button" onclick="exportToPDF()">EXPORT AS PDF</button>
        <a href="Autoencoders_and_VAE.apkg" download class="action-button">DOWNLOAD ANKI DECK</a>
    </div>

    <script>
        function exportToPDF() {
            window.print();
        }
    </script>

</body>
</html>
