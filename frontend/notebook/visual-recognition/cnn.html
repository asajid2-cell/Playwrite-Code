<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CNNs for Image Classification - CMPUT 328</title>
    <style>
*{scrollbar-width:none!important;-ms-overflow-style:none!important}
*::-webkit-scrollbar{display:none!important;width:0!important;height:0!important}
*::-webkit-scrollbar-track{display:none!important}
*::-webkit-scrollbar-thumb{display:none!important}
html::-webkit-scrollbar{display:none!important;width:0!important}
html{scrollbar-width:none!important;-ms-overflow-style:none!important}

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Courier New', Consolas, Monaco, monospace;
            background-color: #000000;
            background-image: radial-gradient(circle, #ffffff 1px, transparent 1px), radial-gradient(circle, #ffffff 1px, transparent 1px);
            background-size: 50px 50px, 80px 80px;
            background-position: 0 0, 40px 40px;
            background-attachment: fixed;
            color: #ffffff;
            line-height: 1.6;
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }

        h1, h2, h3, h4, h5, h6 {
            text-align: left;
            margin: 30px 0 15px 0;
            font-weight: bold;
            letter-spacing: 1px;
        }

        h1 {
            font-size: 2.5em;
            border-bottom: 3px solid #ffffff;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }

        h2 {
            font-size: 2em;
            border-bottom: 2px solid #ffffff;
            padding-bottom: 8px;
            margin-top: 50px;
        }

        h3 {
            font-size: 1.5em;
            border-left: 5px solid #ffffff;
            padding-left: 15px;
        }

        h4 {
            font-size: 1.2em;
            text-decoration: underline;
        }

        p {
            margin: 15px 0;
            text-align: left;
        }

        ul, ol {
            margin: 15px 0;
            padding-left: 40px;
            text-align: left;
        }

        li {
            margin: 8px 0;
        }

        code {
            background-color: #1a1a1a;
            color: #ffffff;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', Consolas, Monaco, monospace;
            border: 1px solid #333333;
        }

        pre {
            background-color: #1a1a1a;
            color: #ffffff;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid #333333;
            text-align: left;
        }

        pre code {
            background: none;
            border: none;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: #0a0a0a;
        }

        th, td {
            border: 1px solid #ffffff;
            padding: 12px;
            text-align: left;
        }

        th {
            background-color: #1a1a1a;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: #0f0f0f;
        }

        .toc {
            background-color: #1a1a1a;
            border: 2px solid #ffffff;
            padding: 20px;
            margin: 30px 0;
        }

        .toc h2 {
            margin-top: 0;
            border: none;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 20px;
        }

        .toc a {
            color: #ffffff;
            text-decoration: none;
            border-bottom: 1px dotted #ffffff;
        }

        .toc a:hover {
            background-color: #ffffff;
            color: #000000;
        }

        .section {
            margin: 40px 0;
            padding: 20px 0;
        }

        .highlight {
            background-color: #1a1a1a;
            border-left: 5px solid #ffffff;
            padding: 15px;
            margin: 20px 0;
        }

        .formula {
            text-align: center;
            font-size: 1.2em;
            margin: 20px 0;
            padding: 15px;
            background-color: #1a1a1a;
            border: 1px solid #ffffff;
        }

        .important {
            font-weight: bold;
            text-decoration: underline;
        }

        .note {
            background-color: #1a1a1a;
            border: 2px solid #ffffff;
            padding: 15px;
            margin: 20px 0;
        }

        .note::before {
            content: "NOTE: ";
            font-weight: bold;
        }

        hr {
            border: none;
            border-top: 1px solid #ffffff;
            margin: 40px 0;
        }

        a {
            color: #ffffff;
            text-decoration: underline;
        }

        a:hover {
            background-color: #ffffff;
            color: #000000;
        }

        .header {
            text-align: center;
            margin-bottom: 50px;
            padding: 30px;
            border: 3px solid #ffffff;
            background-color: #0a0a0a;
        }

        .header h1 {
            border: none;
            margin: 0;
        }

        .header p {
            font-size: 1.2em;
            margin-top: 10px;
            text-align: center;
        }

        .diagram {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            background-color: #1a1a1a;
            border: 1px solid #ffffff;
            font-family: 'Courier New', monospace;
        }

        .ascii-art {
            display: inline-block;
            text-align: left;
        }

        /* Action buttons */
        .action-buttons {
            display: flex;
            gap: 20px;
            margin: 50px 0;
            justify-content: center;
            flex-wrap: wrap;
        }

        .action-button {
            background-color: #000000;
            color: #ffffff;
            border: 2px solid #ffffff;
            padding: 15px 25px;
            text-transform: uppercase;
            letter-spacing: 1px;
            font-size: 1em;
            cursor: pointer;
            min-width: 220px;
            text-align: center;
        }

        .action-button:hover {
            background-color: #ffffff;
            color: #000000;
        }

        .action-button:active {
            transform: translateY(1px);
        }

        @media print {
            .action-buttons {
                display: none;
            }
            body {
                background-color: #ffffff;
                color: #000000;
            }
            .header {
                background-color: #ffffff;
                border: 3px solid #000000;
            }
            .toc {
                background-color: #ffffff;
                border: 2px solid #000000;
            }
            .highlight, .note, .diagram {
                background-color: #f5f5f5;
                border: 1px solid #000000;
            }
            code, pre {
                background-color: #f5f5f5;
                color: #000000;
                border: 1px solid #000000;
            }
            table {
                background-color: #ffffff;
            }
            th {
                background-color: #e0e0e0;
            }
            tr:nth-child(even) {
                background-color: #f5f5f5;
            }
            a {
                color: #000000;
            }
        }
    
        .nav-back {
            position: fixed;
            top: 20px;
            left: 20px;
            background-color: #000000;
            color: #ffffff;
            border: 2px solid #ffffff;
            padding: 10px 16px;
            text-decoration: none;
            text-transform: uppercase;
            letter-spacing: 0.15em;
            font-size: 0.85rem;
            z-index: 1000;
            transition: all 0.2s ease;
        }

        .nav-back:hover {
            background-color: #ffffff;
            color: #000000;
        }

    </style>
</head>
<body>
    <a href="./" class="nav-back">&#8592; Back to Topics</a>
    <div class="header">
        <h1>CONVOLUTIONAL NEURAL NETWORKS</h1>
        <p>FOR IMAGE CLASSIFICATION</p>
        <p>CMPUT 328 - ASSIGNMENT 3 STUDY GUIDE</p>
    </div>

    <div class="toc">
        <h2>TABLE OF CONTENTS</h2>
        <ul>
            <li><a href="#intro">1. Introduction to CNNs</a></li>
            <li><a href="#why">2. Why CNNs for Images?</a></li>
            <li><a href="#components">3. CNN Architecture Components</a></li>
            <li><a href="#conv">4. Convolutional Layers</a></li>
            <li><a href="#pool">5. Pooling Layers</a></li>
            <li><a href="#activation">6. Activation Functions</a></li>
            <li><a href="#norm">7. Normalization Techniques</a></li>
            <li><a href="#reg">8. Regularization in CNNs</a></li>
            <li><a href="#aug">9. Data Augmentation</a></li>
            <li><a href="#train">10. Training CNNs</a></li>
            <li><a href="#cifar">11. CIFAR-10 Dataset</a></li>
            <li><a href="#eval">12. Model Evaluation</a></li>
            <li><a href="#compare">13. CNN vs Fully Connected Networks</a></li>
            <li><a href="#impl">14. Implementation Guide</a></li>
            <li><a href="#pitfalls">15. Common Pitfalls and Solutions</a></li>
        </ul>
    </div>

    <div class="section" id="intro">
        <h2>1. INTRODUCTION TO CNNs</h2>

        <h3>What is a Convolutional Neural Network?</h3>
        <p>A Convolutional Neural Network (CNN) is a specialized type of neural network designed for processing grid-like data, particularly images. CNNs are inspired by the visual cortex of animals and are the foundation of modern computer vision.</p>

        <h3>Key Characteristics</h3>
        <ul>
            <li><span class="important">Spatial hierarchy:</span> CNNs learn hierarchical patterns from low-level features (edges) to high-level concepts (objects)</li>
            <li><span class="important">Parameter sharing:</span> Same filters are applied across the entire image</li>
            <li><span class="important">Translation invariance:</span> Can detect features regardless of their position in the image</li>
            <li><span class="important">Sparse connectivity:</span> Each neuron connects only to a local region of the input</li>
        </ul>

        <div class="diagram">
            <pre class="ascii-art">
INPUT IMAGE (32×32×3)
        ↓
    ┌───────┐
    │  CONV │ → Detect edges, colors
    └───────┘
        ↓
    ┌───────┐
    │  POOL │ → Reduce size
    └───────┘
        ↓
    ┌───────┐
    │  CONV │ → Detect shapes
    └───────┘
        ↓
    ┌───────┐
    │  POOL │ → Reduce size
    └───────┘
        ↓
    ┌───────┐
    │  CONV │ → Detect objects
    └───────┘
        ↓
    ┌───────┐
    │   FC  │ → Classification
    └───────┘
        ↓
    OUTPUT (10 classes)
            </pre>
        </div>
    </div>

    <div class="section" id="why">
        <h2>2. WHY CNNs FOR IMAGES?</h2>

        <h3>The Problem with Fully Connected Networks</h3>

        <div class="highlight">
            <p><strong>When we flatten an image for a fully connected network:</strong></p>
            <pre><code>Input: 32×32×3 CIFAR-10 image
Flattened: 3,072 dimensional vector
Hidden layer: 1,024 neurons
Parameters: 3,072 × 1,024 = 3,145,728 parameters!</code></pre>
        </div>

        <h3>Problems with FC Networks</h3>
        <table>
            <tr>
                <th>Problem</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Loss of spatial structure</td>
                <td>Pixels that are spatially close treated same as pixels far apart</td>
            </tr>
            <tr>
                <td>Huge parameter count</td>
                <td>Millions of parameters in first layer alone</td>
            </tr>
            <tr>
                <td>No translation invariance</td>
                <td>Must learn same feature at every possible position</td>
            </tr>
            <tr>
                <td>Overfitting</td>
                <td>Too many parameters lead to poor generalization</td>
            </tr>
        </table>

        <h3>How CNNs Solve These Problems</h3>
        <ul>
            <li><strong>Preserve spatial structure:</strong> Input shape [batch, channels, height, width] - never flattened!</li>
            <li><strong>Parameter efficiency:</strong> 3×3 conv with 64 filters = only 1,792 parameters</li>
            <li><strong>Translation invariance:</strong> Same filter detects edges everywhere</li>
            <li><strong>Better generalization:</strong> Fewer parameters = less overfitting</li>
        </ul>
    </div>

    <div class="section" id="components">
        <h2>3. CNN ARCHITECTURE COMPONENTS</h2>

        <div class="diagram">
            <pre class="ascii-art">
┌────────────────────────────────────────┐
│          STANDARD CNN ARCHITECTURE      │
├────────────────────────────────────────┤
│  Input Image (32×32×3)                 │
│           ↓                             │
│  ┌──────────────────────┐               │
│  │ Conv → ReLU → Pool   │ ×N            │
│  └──────────────────────┘               │
│           ↓                             │
│  ┌──────────────────────┐               │
│  │ Conv → ReLU → Pool   │ ×M            │
│  └──────────────────────┘               │
│           ↓                             │
│  Global Average Pooling                │
│           ↓                             │
│  Fully Connected → Output (10 classes) │
└────────────────────────────────────────┘
            </pre>
        </div>

        <h3>Layer Types</h3>
        <ol>
            <li><strong>Convolutional layers:</strong> Extract spatial features</li>
            <li><strong>Activation layers:</strong> Introduce non-linearity (ReLU)</li>
            <li><strong>Pooling layers:</strong> Downsample spatial dimensions</li>
            <li><strong>Normalization layers:</strong> Stabilize training (BatchNorm)</li>
            <li><strong>Dropout layers:</strong> Regularization</li>
            <li><strong>Fully connected layers:</strong> Final classification</li>
        </ol>
    </div>

    <div class="section" id="conv">
        <h2>4. CONVOLUTIONAL LAYERS</h2>

        <h3>What is Convolution?</h3>
        <p>Convolution is a mathematical operation that slides a small filter (kernel) over an input to produce a feature map.</p>

        <div class="formula">
            Output[i,j] = Σ Σ Input[i+m, j+n] × Kernel[m,n]
        </div>

        <h3>Key Parameters</h3>
        <table>
            <tr>
                <th>Parameter</th>
                <th>Description</th>
                <th>Common Values</th>
            </tr>
            <tr>
                <td>Kernel size (k)</td>
                <td>Size of the sliding window</td>
                <td>3×3, 5×5, 7×7</td>
            </tr>
            <tr>
                <td>Stride (s)</td>
                <td>How many pixels to slide</td>
                <td>1, 2</td>
            </tr>
            <tr>
                <td>Padding (p)</td>
                <td>Add zeros around border</td>
                <td>0, 1, 2</td>
            </tr>
            <tr>
                <td>Filters (out_channels)</td>
                <td>Number of output feature maps</td>
                <td>32, 64, 128, 256</td>
            </tr>
        </table>

        <h3>Output Size Formula</h3>
        <div class="formula">
            output_size = ⌊(input_size + 2×padding - kernel_size) / stride⌋ + 1
        </div>

        <div class="note">
            For a 3×3 kernel with padding=1 and stride=1, the output size equals input size!
        </div>

        <h3>How Filters Work</h3>
        <ul>
            <li><strong>Early layers (low-level):</strong> Edge detectors, color blobs, simple textures</li>
            <li><strong>Middle layers (mid-level):</strong> Corners, curves, simple shapes</li>
            <li><strong>Deep layers (high-level):</strong> Object parts, complex patterns, semantic concepts</li>
        </ul>

        <h3>Parameter Count</h3>
        <div class="formula">
            Parameters = (kernel_h × kernel_w × in_channels + 1) × out_channels
        </div>

        <pre><code>Example: Conv2d(3, 64, kernel_size=3)
= (3 × 3 × 3 + 1) × 64 = 1,792 parameters</code></pre>
    </div>

    <div class="section" id="pool">
        <h2>5. POOLING LAYERS</h2>

        <h3>Purpose of Pooling</h3>
        <ol>
            <li>Reduce spatial dimensions → decrease computational cost</li>
            <li>Increase receptive field → each neuron "sees" more</li>
            <li>Add translation invariance → small shifts don't change output</li>
            <li>Reduce overfitting → fewer parameters in subsequent layers</li>
        </ol>

        <h3>Types of Pooling</h3>
        <table>
            <tr>
                <th>Type</th>
                <th>Operation</th>
                <th>Use Case</th>
            </tr>
            <tr>
                <td>MaxPool2d(2)</td>
                <td>Takes maximum value</td>
                <td>Most common, preserves strong activations</td>
            </tr>
            <tr>
                <td>AvgPool2d(2)</td>
                <td>Takes average value</td>
                <td>Smoother downsampling</td>
            </tr>
            <tr>
                <td>AdaptiveAvgPool2d(1)</td>
                <td>Global average pooling</td>
                <td>Before final classifier, replaces flatten</td>
            </tr>
        </table>

        <div class="diagram">
            <pre class="ascii-art">
Size Reduction with MaxPool(2):
32×32 → MaxPool → 16×16
16×16 → MaxPool → 8×8
 8×8  → MaxPool → 4×4
 4×4  → MaxPool → 2×2
            </pre>
        </div>
    </div>

    <div class="section" id="activation">
        <h2>6. ACTIVATION FUNCTIONS</h2>

        <h3>Why Activation Functions?</h3>
        <div class="highlight">
            <p>Without activation functions, stacking layers is useless:</p>
            <p class="formula">Linear → Linear → Linear ≡ Single Linear Layer</p>
            <p>Activations introduce <strong>non-linearity</strong>, allowing networks to learn complex patterns.</p>
        </div>

        <h3>ReLU (Rectified Linear Unit)</h3>
        <div class="formula">
            ReLU(x) = max(0, x)
        </div>

        <h4>Advantages:</h4>
        <ul>
            <li>Fast to compute</li>
            <li>Helps with vanishing gradients</li>
            <li>Sparse activations (many zeros)</li>
            <li>Works well in practice</li>
        </ul>

        <h4>Disadvantages:</h4>
        <ul>
            <li>"Dying ReLU" problem: neurons can get stuck at 0</li>
            <li>Not zero-centered</li>
        </ul>

        <h3>Other Activations</h3>
        <table>
            <tr>
                <th>Function</th>
                <th>Formula</th>
                <th>Use Case</th>
            </tr>
            <tr>
                <td>Leaky ReLU</td>
                <td>max(0.01x, x)</td>
                <td>Prevents dying neurons</td>
            </tr>
            <tr>
                <td>Sigmoid</td>
                <td>1 / (1 + e^(-x))</td>
                <td>Binary classification output</td>
            </tr>
            <tr>
                <td>Tanh</td>
                <td>(e^x - e^(-x)) / (e^x + e^(-x))</td>
                <td>Zero-centered alternative to sigmoid</td>
            </tr>
            <tr>
                <td>Softmax</td>
                <td>e^(x_i) / Σ e^(x_j)</td>
                <td>Multi-class classification output</td>
            </tr>
        </table>
    </div>

    <div class="section" id="norm">
        <h2>7. NORMALIZATION TECHNIQUES</h2>

        <h3>Batch Normalization</h3>
        <p><strong>Purpose:</strong> Normalize layer inputs to have mean=0, std=1</p>

        <div class="formula">
            BatchNorm(x) = γ × (x - μ_batch) / √(σ²_batch + ε) + β
        </div>

        <h4>Benefits:</h4>
        <ul>
            <li>Faster convergence</li>
            <li>Allows higher learning rates</li>
            <li>Less sensitive to initialization</li>
            <li>Acts as regularization</li>
        </ul>

        <h3>Typical Placement</h3>
        <pre><code>Conv → BatchNorm → ReLU

nn.Conv2d(64, 128, 3, padding=1),
nn.BatchNorm2d(128),
nn.ReLU(inplace=True)</code></pre>

        <div class="note">
            During training: uses batch statistics<br>
            During inference: uses running averages computed during training
        </div>
    </div>

    <div class="section" id="reg">
        <h2>8. REGULARIZATION IN CNNs</h2>

        <h3>Dropout</h3>
        <p><strong>Purpose:</strong> Prevent overfitting by randomly dropping activations</p>

        <ul>
            <li>During training: randomly set activations to 0 with probability p</li>
            <li>During inference: scale activations by (1-p)</li>
        </ul>

        <h3>Typical Usage</h3>
        <pre><code># Higher dropout in FC layers
nn.Dropout(0.5)

# Lower dropout after conv layers
nn.Dropout2d(0.25)</code></pre>

        <h3>Weight Decay (L2 Regularization)</h3>
        <div class="formula">
            Loss_total = Loss_original + λ × Σ(w²)
        </div>

        <pre><code>optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=5e-4  # λ = 0.0005
)</code></pre>

        <h3>Label Smoothing</h3>
        <pre><code>criterion = nn.CrossEntropyLoss(label_smoothing=0.1)</code></pre>
        <p>Prevents overconfident predictions and improves calibration</p>
    </div>

    <div class="section" id="aug">
        <h2>9. DATA AUGMENTATION</h2>

        <h3>Why Data Augmentation?</h3>
        <ul>
            <li>Increase effective dataset size</li>
            <li>Improve generalization</li>
            <li>Reduce overfitting</li>
            <li>Better calibration</li>
        </ul>

        <h3>Common Augmentations for CIFAR-10</h3>
        <table>
            <tr>
                <th>Augmentation</th>
                <th>Effect</th>
                <th>Expected Gain</th>
            </tr>
            <tr>
                <td>RandomCrop(32, padding=4)</td>
                <td>Translation invariance</td>
                <td>+2-3%</td>
            </tr>
            <tr>
                <td>RandomHorizontalFlip()</td>
                <td>Left-right symmetry</td>
                <td>+1-2%</td>
            </tr>
            <tr>
                <td>ColorJitter(0.2, 0.2, 0.2, 0.1)</td>
                <td>Lighting robustness</td>
                <td>+1-2%</td>
            </tr>
            <tr>
                <td>RandomErasing(p=0.15)</td>
                <td>Occlusion robustness</td>
                <td>+0.5-1%</td>
            </tr>
        </table>

        <h3>Complete Pipeline</h3>
        <pre><code>train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.4914, 0.4822, 0.4465),
        std=(0.2470, 0.2435, 0.2616)
    ),
    transforms.RandomErasing(p=0.15),
])</code></pre>

        <div class="note">
            NEVER augment the test set! Evaluate on clean data only.
        </div>
    </div>

    <div class="section" id="train">
        <h2>10. TRAINING CNNs</h2>

        <h3>Loss Functions</h3>
        <pre><code>criterion = nn.CrossEntropyLoss(label_smoothing=0.05)</code></pre>
        <ul>
            <li>Combines LogSoftmax + NLLLoss</li>
            <li>Expects raw logits (before softmax)</li>
            <li>Formula: -log(softmax(logits)[target_class])</li>
        </ul>

        <h3>Optimizers</h3>
        <table>
            <tr>
                <th>Optimizer</th>
                <th>Pros</th>
                <th>Cons</th>
            </tr>
            <tr>
                <td>Adam / AdamW</td>
                <td>Fast convergence, adaptive LR</td>
                <td>Can generalize slightly worse</td>
            </tr>
            <tr>
                <td>SGD + Momentum</td>
                <td>Better generalization</td>
                <td>Requires careful tuning</td>
            </tr>
        </table>

        <h3>Learning Rate Schedules</h3>
        <pre><code># Cosine Annealing (smooth decrease)
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=epochs, eta_min=1e-6
)

# Step LR (decrease by factor every N epochs)
scheduler = optim.lr_scheduler.StepLR(
    optimizer, step_size=30, gamma=0.1
)

# ReduceLROnPlateau (adaptive)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)</code></pre>

        <h3>Gradient Clipping</h3>
        <pre><code>torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_norm=1.0
)</code></pre>
        <p>Prevents exploding gradients, especially useful with high learning rates</p>
    </div>

    <div class="section" id="cifar">
        <h2>11. CIFAR-10 DATASET</h2>

        <h3>Dataset Overview</h3>
        <table>
            <tr>
                <th>Property</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Total Images</td>
                <td>60,000 (50k train, 10k test)</td>
            </tr>
            <tr>
                <td>Resolution</td>
                <td>32×32 pixels</td>
            </tr>
            <tr>
                <td>Channels</td>
                <td>3 (RGB)</td>
            </tr>
            <tr>
                <td>Classes</td>
                <td>10 (balanced)</td>
            </tr>
        </table>

        <h3>Class Distribution</h3>
        <ol>
            <li>Airplane</li>
            <li>Automobile</li>
            <li>Bird</li>
            <li>Cat</li>
            <li>Deer</li>
            <li>Dog</li>
            <li>Frog</li>
            <li>Horse</li>
            <li>Ship</li>
            <li>Truck</li>
        </ol>

        <h3>Data Normalization</h3>
        <pre><code># CIFAR-10 statistics
mean = (0.4914, 0.4822, 0.4465)  # RGB
std = (0.2470, 0.2435, 0.2616)

normalize = transforms.Normalize(mean=mean, std=std)</code></pre>

        <div class="note">
            Why normalize? Zero-centered inputs speed up convergence and prevent activation saturation
        </div>

        <h3>Train/Val Split</h3>
        <pre><code># Typical split: 45k train / 5k val / 10k test
val_size = 5000
train_indices = list(range(0, 50000 - val_size))
val_indices = list(range(50000 - val_size, 50000))</code></pre>
    </div>

    <div class="section" id="eval">
        <h2>12. MODEL EVALUATION</h2>

        <h3>Key Metrics</h3>
        <table>
            <tr>
                <th>Metric</th>
                <th>Formula</th>
                <th>Interpretation</th>
            </tr>
            <tr>
                <td>Accuracy</td>
                <td>correct / total</td>
                <td>Overall correctness</td>
            </tr>
            <tr>
                <td>Loss</td>
                <td>CrossEntropyLoss</td>
                <td>Confidence-aware error</td>
            </tr>
            <tr>
                <td>Confidence</td>
                <td>max(softmax(logits))</td>
                <td>Model certainty</td>
            </tr>
        </table>

        <h3>Evaluation Function</h3>
        <pre><code>def evaluate(model, dataloader, criterion, device):
    model.eval()  # IMPORTANT!
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs = inputs.to(device)
            targets = targets.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)

            running_loss += loss.item() * inputs.size(0)
            predictions = outputs.argmax(dim=1)
            correct += (predictions == targets).sum().item()
            total += targets.size(0)

    return running_loss / total, correct / total</code></pre>

        <div class="note">
            Always call model.eval() before evaluation to disable dropout and switch BatchNorm to eval mode!
        </div>
    </div>

    <div class="section" id="compare">
        <h2>13. CNN vs FULLY CONNECTED NETWORKS</h2>

        <h3>Parameter Comparison</h3>
        <table>
            <tr>
                <th>Network Type</th>
                <th>First Layer</th>
                <th>Parameters</th>
            </tr>
            <tr>
                <td>Fully Connected</td>
                <td>3,072 → 1,024</td>
                <td>3,145,728</td>
            </tr>
            <tr>
                <td>CNN</td>
                <td>3→64, kernel=3×3</td>
                <td>1,792</td>
            </tr>
            <tr>
                <td><strong>Ratio</strong></td>
                <td></td>
                <td><strong>1,754:1</strong></td>
            </tr>
        </table>

        <h3>Spatial Awareness</h3>
        <div class="highlight">
            <p><strong>Fully Connected:</strong></p>
            <ul>
                <li>Flattens image: [batch, 3, 32, 32] → [batch, 3072]</li>
                <li>No notion of "nearby pixels"</li>
                <li>Must relearn patterns at different positions</li>
            </ul>

            <p><strong>CNN:</strong></p>
            <ul>
                <li>Preserves structure: [batch, 3, 32, 32] → [batch, 64, 32, 32]</li>
                <li>Adjacent pixels processed together</li>
                <li>Translation invariance built-in</li>
            </ul>
        </div>

        <h3>Typical CIFAR-10 Results</h3>
        <table>
            <tr>
                <th>Model</th>
                <th>Parameters</th>
                <th>Test Accuracy</th>
            </tr>
            <tr>
                <td>Random Guessing</td>
                <td>-</td>
                <td>10%</td>
            </tr>
            <tr>
                <td>3-layer FC</td>
                <td>3.8M</td>
                <td>55-60%</td>
            </tr>
            <tr>
                <td>Simple CNN</td>
                <td>0.6M</td>
                <td>75-80%</td>
            </tr>
            <tr>
                <td>CNN + Augmentation</td>
                <td>1.2M</td>
                <td>85-90%</td>
            </tr>
            <tr>
                <td>ResNet-18</td>
                <td>11M</td>
                <td>92-95%</td>
            </tr>
        </table>
    </div>

    <div class="section" id="impl">
        <h2>14. IMPLEMENTATION GUIDE</h2>

        <h3>Model Architecture</h3>
        <pre><code>class CIFAR10CNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()

        self.features = nn.Sequential(
            # Block 1: 32×32 → 16×16
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            # Block 2: 16×16 → 8×8
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.35),

            # Block 3: 8×8 → 1×1
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x</code></pre>

        <h3>Training Setup</h3>
        <pre><code># Model, loss, optimizer
model = CIFAR10CNN().to(device)
criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# Training loop
for epoch in range(num_epochs):
    model.train()
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

    scheduler.step()</code></pre>
    </div>

    <div class="section" id="pitfalls">
        <h2>15. COMMON PITFALLS AND SOLUTIONS</h2>

        <h3>Pitfall 1: Flattening at Input</h3>
        <div class="highlight">
            <p><strong>WRONG:</strong></p>
            <pre><code>def forward(self, x):
    x = x.view(x.size(0), -1)  # DESTROYS SPATIAL STRUCTURE!
    x = self.conv1(x)  # This will crash</code></pre>

            <p><strong>CORRECT:</strong></p>
            <pre><code>def forward(self, x):
    # x shape: [batch, 3, 32, 32] - keep spatial structure!
    x = self.features(x)
    x = self.classifier(x)</code></pre>
        </div>

        <h3>Pitfall 2: Augmenting Test Data</h3>
        <div class="highlight">
            <p><strong>WRONG:</strong></p>
            <pre><code>test_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),  # NO!
    transforms.ToTensor(),
])</code></pre>

            <p><strong>CORRECT:</strong></p>
            <pre><code>test_transform = transforms.Compose([
    transforms.ToTensor(),  # No augmentation!
    transforms.Normalize(mean, std),
])</code></pre>
        </div>

        <h3>Pitfall 3: Not Using .eval()</h3>
        <div class="highlight">
            <p><strong>WRONG:</strong></p>
            <pre><code>with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)  # BatchNorm/Dropout still active!</code></pre>

            <p><strong>CORRECT:</strong></p>
            <pre><code>model.eval()  # IMPORTANT!
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)</code></pre>
        </div>

        <h3>Common Training Issues</h3>
        <table>
            <tr>
                <th>Symptom</th>
                <th>Problem</th>
                <th>Solution</th>
            </tr>
            <tr>
                <td>Loss = NaN</td>
                <td>Learning rate too high</td>
                <td>Lower LR, add gradient clipping</td>
            </tr>
            <tr>
                <td>Train >> Val accuracy</td>
                <td>Overfitting</td>
                <td>More dropout, augmentation, weight decay</td>
            </tr>
            <tr>
                <td>Both train/val low</td>
                <td>Underfitting</td>
                <td>Increase capacity, train longer</td>
            </tr>
            <tr>
                <td>Loss oscillates wildly</td>
                <td>LR too high or batch size too small</td>
                <td>Lower LR, increase batch size</td>
            </tr>
        </table>
    </div>

    <hr>

    <div class="section">
        <h2>SUMMARY</h2>

        <h3>Key Takeaways</h3>
        <ol>
            <li>CNNs preserve spatial structure - NEVER flatten at input</li>
            <li>Convolutions are parameter-efficient - same filter reused across image</li>
            <li>Use BatchNorm + ReLU after Conv layers for stable training</li>
            <li>Data augmentation is crucial - easily +5-10% accuracy</li>
            <li>Always normalize inputs using dataset statistics</li>
            <li>Don't augment test set - evaluate on clean data</li>
            <li>Use proper train/val/test splits - 45k/5k/10k for CIFAR-10</li>
            <li>Monitor both train and val metrics - detect overfitting</li>
            <li>Save best model, not last epoch</li>
            <li>Set seeds for reproducibility</li>
        </ol>

        <h3>Expected Results</h3>
        <table>
            <tr>
                <th>Configuration</th>
                <th>Expected Test Accuracy</th>
            </tr>
            <tr>
                <td>Simple CNN, no augmentation</td>
                <td>70-75%</td>
            </tr>
            <tr>
                <td>CNN + basic augmentation</td>
                <td>80-85%</td>
            </tr>
            <tr>
                <td>CNN + full augmentation pipeline</td>
                <td>85-90%</td>
            </tr>
        </table>
    </div>

    <hr>

    <div style="text-align: center; margin: 50px 0; padding: 30px; border: 2px solid #ffffff;">
        <p style="font-size: 1.5em; text-align: center;"><strong>END OF LESSON</strong></p>
        <p style="text-align: center;">CMPUT 328 - VISUAL RECOGNITION</p>
        <p style="text-align: center;">ASSIGNMENT 3: CNNs FOR IMAGE CLASSIFICATION</p>
    </div>

    <!-- Action Buttons -->
    <div class="action-buttons">
        <button class="action-button" onclick="exportToPDF()">EXPORT AS PDF</button>
        <a href="/Study/CNN/CNN_CIFAR10_Image_Classification.apkg" download class="action-button">DOWNLOAD ANKI DECK</a>
    </div>

    <script>
        function exportToPDF() {
            // Simple print-to-PDF functionality
            window.print();
        }
    </script>

<script src="../../js/persistent-player.js" defer></script>
</body>
</html>
